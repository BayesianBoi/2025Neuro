{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Multi-Participant MEG Preprocessing Pipeline\n",
    "\n",
    "\n",
    "## IMPORTANT: In order to parallel run ICA and other parallelised processes, choose at least a 16-core machine. After finished, return to smaller machine. \n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Load all participant data\n",
    "2. Preprocessing (filtering, artifact detection)\n",
    "3. Mark bad channels per participant\n",
    "4. Continue with epoching, evoked responses, and source reconstruction\n",
    "\n",
    "**Note:** All code is kept as notebook cells - no functions or scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe25e7",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58268252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threading for parallel processing\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTALLS\n",
    "\n",
    "%pip install python-picard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mne.preprocessing import ICA\n",
    "from joblib import Parallel, delayed\n",
    "import mne\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "set_paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATHS\n",
    "\n",
    "data_folder = Path('/work/MEG_data/workshop_data')\n",
    "subjects_dir = '/work/freesurfer'\n",
    "behaviour_path = '/work/MEG_data/workshop_data/behavioural_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_participants",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIND ALL PARTICIPANTS - We'll loop through all participant folders and load their MEG data recursively\n",
    "\n",
    "# Dictionary to store raw data for each participant\n",
    "participant_data = {}\n",
    "\n",
    "# Loop through each participant folder\n",
    "for participant_folder in sorted(data_folder.iterdir()):\n",
    "    if participant_folder.is_dir() and not participant_folder.name.startswith('.'):\n",
    "        participant_id = participant_folder.name\n",
    "        print(f\"\\n=== Loading {participant_id} ===\")\n",
    "        fif_files = sorted(participant_folder.rglob('*_raw.fif'))\n",
    "        \n",
    "        if fif_files:\n",
    "            print(f\"  Found {len(fif_files)} file(s)\")\n",
    "            \n",
    "            raw_list = []\n",
    "            for fif_file in fif_files:\n",
    "                print(f\"    Loading: {fif_file.name}\")\n",
    "                raw = mne.io.read_raw_fif(fif_file, preload=True)\n",
    "                raw_list.append(raw)\n",
    "            \n",
    "            if len(raw_list) > 1:\n",
    "                raw_combined = mne.concatenate_raws(raw_list)\n",
    "                print(f\"  Concatenated {len(raw_list)} files\")\n",
    "            else:\n",
    "                raw_combined = raw_list[0]\n",
    "            \n",
    "            # storing in dict\n",
    "            participant_data[participant_id] = raw_combined\n",
    "        else:\n",
    "            print(f\"  No .fif files found\")\n",
    "\n",
    "print(f\"\\n\\nTotal participants loaded: {len(participant_data)}\")\n",
    "print(f\"Participant IDs: {list(participant_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_section",
   "metadata": {},
   "source": [
    "## 2. Inspecting and improving data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_first",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSPECT ONE PARTICIPANT FIRST\n",
    "\n",
    "first_participant = list(participant_data.keys())[0]\n",
    "raw_example = participant_data[first_participant]\n",
    "\n",
    "print(f\"Inspecting participant: {first_participant}\")\n",
    "print(f\"Duration: {raw_example.times[-1]:.1f} seconds\")\n",
    "print(f\"Sampling frequency: {raw_example.info['sfreq']} Hz\")\n",
    "print(f\"Number of channels: {len(raw_example.ch_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_psd_first",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE PSD FOR FIRST PARTICIPANT - HPI frequencies\n",
    "\n",
    "raw_example.compute_psd().plot()\n",
    "plt.suptitle(f'PSD - {first_participant} (BEFORE filtering)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter_section",
   "metadata": {},
   "source": [
    "#### Applying filtering to remove HPI frequencies\n",
    "\n",
    "(Typical HPI frequencies are around 150-350 Hz.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter_all",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER ALL PARTICIPANTS\n",
    "\n",
    "for participant_id, raw in participant_data.items():\n",
    "    print(f\"Filtering {participant_id}...\")\n",
    "    raw.filter(l_freq=1, h_freq=40) \n",
    "    print(f\"  Done!\")\n",
    "\n",
    "print(\"\\nAll participants filtered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_filtering",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK FILTERING RESULT\n",
    "\n",
    "# Plot PSD again for the first participant to verify filtering worked\n",
    "raw_example.compute_psd().plot()\n",
    "plt.suptitle(f'PSD - {first_participant} (AFTER filtering)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad_channels_section",
   "metadata": {},
   "source": [
    "#### Identifyng bad channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_participant_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANUAL INSPECTION - PARTICIPANT 1\n",
    "\n",
    "# Get first participant\n",
    "participant_ids = list(participant_data.keys())\n",
    "current_id = participant_ids[0]\n",
    "raw_current = participant_data[current_id]\n",
    "\n",
    "print(f\"Inspecting: {current_id}\")\n",
    "print(f\"Currently marked bad: {raw_current.info['bads']}\")\n",
    "print(\"\\nClick channel names to mark as bad. Close window when done.\")\n",
    "\n",
    "# plotting for manual inspection\n",
    "raw_current.plot(duration=10.0, n_channels=30)\n",
    "\n",
    "print(f\"\\nFinal bad channels for {current_id}: {raw_current.info['bads']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_participant_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANUAL INSPECTION - PARTICIPANT 2\n",
    "\n",
    "if len(participant_ids) > 1:\n",
    "    current_id = participant_ids[1]\n",
    "    raw_current = participant_data[current_id]\n",
    "    \n",
    "    print(f\"Inspecting: {current_id}\")\n",
    "    print(f\"Currently marked bad: {raw_current.info['bads']}\")\n",
    "    \n",
    "    raw_current.plot(duration=10.0, n_channels=20)\n",
    "    \n",
    "    print(f\"\\nFinal bad channels for {current_id}: {raw_current.info['bads']}\")\n",
    "else:\n",
    "    print(\"Only one participant in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_participant_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANUAL INSPECTION - PARTICIPANT 3\n",
    "\n",
    "if len(participant_ids) > 2:\n",
    "    current_id = participant_ids[2]\n",
    "    raw_current = participant_data[current_id]\n",
    "    \n",
    "    print(f\"Inspecting: {current_id}\")\n",
    "    print(f\"Currently marked bad: {raw_current.info['bads']}\")\n",
    "    \n",
    "    raw_current.plot(duration=10.0, n_channels=30, scalings='auto', block=True)\n",
    "    \n",
    "    print(f\"\\nFinal bad channels for {current_id}: {raw_current.info['bads']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_remaining",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOOP THROUGH REMAINING PARTICIPANTS\n",
    "\n",
    "for i, participant_id in enumerate(participant_ids[3:], start=4):\n",
    "    raw_current = participant_data[participant_id]\n",
    "    \n",
    "    print(f\"\\n=== Inspecting Participant {i}: {participant_id} ===\")\n",
    "    print(f\"Currently marked bad: {raw_current.info['bads']}\")\n",
    "    \n",
    "    raw_current.plot(duration=10.0, n_channels=30, scalings='auto', block=True)\n",
    "    \n",
    "    print(f\"Final bad channels: {raw_current.info['bads']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad_channels_final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINAL BAD CHANNELS SUMMARY\n",
    "\n",
    "print(\"=== Final Bad Channels Summary ===\")\n",
    "print(\"\\n\")\n",
    "for participant_id, raw in participant_data.items():\n",
    "    n_bad = len(raw.info['bads'])\n",
    "    print(f\"{participant_id}: {n_bad} bad channels\")\n",
    "    if n_bad > 0:\n",
    "        print(f\"  Channels: {raw.info['bads']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a2fc4",
   "metadata": {},
   "source": [
    "#### Annotated bad channels are described below, run this if you don't want to go through the manula process again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define bad channels dictionary\n",
    "bad_channels_dict = {\n",
    "    '0164': ['MEG0321'],\n",
    "    '0170': ['MEG0423', 'MEG1443', 'MEG1922', 'MEG1933', 'MEG2621']\n",
    "}\n",
    "\n",
    "# applying to each participant's raw data\n",
    "for participant_id, bad_list in bad_channels_dict.items():\n",
    "    if participant_id in participant_data:\n",
    "        raw = participant_data[participant_id]\n",
    "\n",
    "        raw.info['bads'] = bad_list\n",
    "        print(f\"Marked bad channels for {participant_id}: {bad_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2f77f",
   "metadata": {},
   "source": [
    "#### Running ICA + interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a934de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ICA with parallelisation \n",
    "\n",
    "# ocnfig\n",
    "DRIFT_SEC = 2.0          # exclude first 2 s from detection\n",
    "N_JOBS    = 8             # 8 participants\n",
    "SAVE_DIR  = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/ICA_cleaned\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# core function\n",
    "def run_ica(raw, n_components=0.95, decim=3, rng=97):\n",
    "    \"\"\"\n",
    "    Fit ICA on drift-free data, detect EOG/ECG on the same window, apply to full raw.\n",
    "    \"\"\"\n",
    "    raw_fit = raw.copy().crop(tmin=DRIFT_SEC)\n",
    "    picks = mne.pick_types(raw_fit.info, meg=True, eog=False, ecg=False, exclude='bads')\n",
    "\n",
    "    ica = ICA(n_components=n_components, method='picard',\n",
    "              random_state=rng, max_iter='auto')\n",
    "    ica.fit(raw_fit, picks=picks, decim=decim,\n",
    "            reject=dict(mag=5e-12, grad=4000e-13))\n",
    "\n",
    "    # detecting artifacts\n",
    "    raw_det = raw.copy().crop(tmin=DRIFT_SEC)\n",
    "\n",
    "    # EOG (blinks + eye movements)\n",
    "    eog_inds, _ = ica.find_bads_eog(raw_det)\n",
    "\n",
    "    # ECG (heartbeat)\n",
    "    try:\n",
    "        ecg_inds, _ = ica.find_bads_ecg(raw_det, method='ctps')\n",
    "    except Exception:\n",
    "        ecg_inds, _ = ica.find_bads_ecg(raw_det, method='correlation')\n",
    "\n",
    "    exclude = sorted(set(list(eog_inds) + list(ecg_inds)))\n",
    "    ica.exclude = exclude\n",
    "\n",
    "    # applying to full recording and interpolating bads \n",
    "    raw_clean = ica.apply(raw.copy())\n",
    "    raw_clean.interpolate_bads(reset_bads=True)\n",
    "\n",
    "    return ica, raw_clean, exclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paralellisation\n",
    "def run_all_ica(participant_data, n_jobs=N_JOBS):\n",
    "    \"\"\"Run ICA for all participants in parallel.\"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(run_ica)(raw) for raw in participant_data.values()\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pplying ICA to every participant\n",
    "ica_results = run_all_ica(participant_data, n_jobs=N_JOBS)\n",
    "\n",
    "participant_ica = {}\n",
    "for (pid, (ica, raw_clean, exclude)) in zip(participant_data.keys(), ica_results):\n",
    "    participant_data[pid] = raw_clean\n",
    "    participant_ica[pid] = ica\n",
    "    print(f\"[{pid}] excluded components: {exclude}\")\n",
    "\n",
    "    raw_clean.save(f\"{SAVE_DIR}/{pid}_cleaned_raw.fif\", overwrite=True)\n",
    "    ica.save(f\"{SAVE_DIR}/{pid}_ica.fif\")\n",
    "\n",
    "print(\"\\n✓ ICA complete and files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting number, mean and sd of removed ICA channels (for report)\n",
    "\n",
    "ICA_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/ICA_cleaned\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "# inding all ICA files \n",
    "ica_files = sorted(glob.glob(os.path.join(ICA_DIR, \"*_ica.fif\")))\n",
    "if not ica_files:\n",
    "    raise FileNotFoundError(\"No *_ica.fif files found in ICA_cleaned directory.\")\n",
    "\n",
    "for f in ica_files:\n",
    "    pid = os.path.basename(f).split(\"_\")[0]\n",
    "\n",
    "    try:\n",
    "        ica = mne.preprocessing.read_ica(f)\n",
    "        excluded = ica.exclude\n",
    "        n_excluded = len(excluded)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {f}: {e}\")\n",
    "        continue\n",
    "\n",
    "    rows.append({\"pid\": pid, \"n_excluded\": n_excluded})\n",
    "\n",
    "df_ica = pd.DataFrame(rows).sort_values(\"pid\").reset_index(drop=True)\n",
    "\n",
    "print(df_ica)\n",
    "print(\"\\nTotal ICA components removed:\", df_ica[\"n_excluded\"].sum())\n",
    "print(\"Mean =\", df_ica[\"n_excluded\"].mean())\n",
    "print(\"SD   =\", df_ica[\"n_excluded\"].std(ddof=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d715df",
   "metadata": {},
   "source": [
    "##### Next we are doing some quality checks to ensure that the ICA appropriately filtered both eye blinks and heartbeat artifacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component summary \n",
    "for pid, ica in participant_ica.items():\n",
    "    print(pid, \"components:\", ica.n_components_, \"excluded:\", ica.exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d812fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick quality check\n",
    "\n",
    "\n",
    "def qc_eog_ecg(pid, raw_clean, ica):\n",
    "    rd = raw_clean.copy().crop(tmin=DRIFT_SEC)\n",
    "    eog_inds, eog_scores = ica.find_bads_eog(rd)\n",
    "    ecg_inds, ecg_scores = ica.find_bads_ecg(rd, method='correlation')\n",
    "\n",
    "    eog_scores = np.asarray(eog_scores, float)\n",
    "    ecg_scores = np.asarray(ecg_scores, float)\n",
    "    if eog_scores.ndim > 1:\n",
    "        eog_scores = eog_scores.max(axis=1)\n",
    "    if ecg_scores.ndim > 1:\n",
    "        ecg_scores = ecg_scores.max(axis=1)\n",
    "\n",
    "    return pid, (eog_scores.max() if eog_scores.size else 0.0), (ecg_scores.max() if ecg_scores.size else 0.0)\n",
    "\n",
    "qc_results = Parallel(n_jobs=N_JOBS)(\n",
    "    delayed(qc_eog_ecg)(pid, participant_data[pid], participant_ica[pid])\n",
    "    for pid in participant_data.keys()\n",
    ")\n",
    "\n",
    "for pid, max_eog, max_ecg in qc_results:\n",
    "    print(f\"{pid}  max EOG={max_eog:.3f},  max ECG={max_ecg:.3f}\")\n",
    "\n",
    "# saving qc table \n",
    "qc_df = pd.DataFrame(qc_results, columns=[\"participant\", \"max_EOG\", \"max_ECG\"])\n",
    "qc_df.to_csv(f\"{SAVE_DIR}/qc_scores.csv\", index=False)\n",
    "print(f\"\\n✓ QC scores saved to {SAVE_DIR}/qc_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking one participant \n",
    "\n",
    "pid = '0168'\n",
    "ica = participant_ica[pid]\n",
    "raw = participant_data[pid]\n",
    "ica.plot_components()\n",
    "ica.plot_sources(raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "events_section",
   "metadata": {},
   "source": [
    "## 3. Finding events\n",
    "\n",
    "Extract event triggers from the data for all participants\n",
    "\n",
    "Events themselves are as follows: \n",
    "\n",
    "- Epochs for all participants\n",
    "event_id = {\n",
    "    \"stimulus_0\": 1, \n",
    "    \"stimulus_1\": 3, \n",
    "    \"mask\": 4,\n",
    "    \"response_stimulus_0\": 6, \n",
    "    \"response_stimulus_1\": 8,\n",
    "    \"response_PAS_1\": 10, \n",
    "    \"response_PAS_2\": 12, \n",
    "    \"response_PAS_3\": 14, \n",
    "    \"response_PAS_4\": 16\n",
    "    add back if present: \"response_PAS_4\": 16, \"response_auto\": 32, \"response_PAS_auto\": 64,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8273bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### SETUP \n",
    "\n",
    "\n",
    "# directories\n",
    "DRIFT_SEC  = 2.0\n",
    "SAVE_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds\"\n",
    "GET_DIR    = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/ICA_cleaned\"\n",
    "EVOKED_DIR = f\"{SAVE_DIR}/evokeds\"\n",
    "EVENTS_DIR = f\"{SAVE_DIR}/events\"\n",
    "EPOCHS_DIR = f\"{SAVE_DIR}/epochs\"\n",
    "os.makedirs(EVENTS_DIR, exist_ok=True)\n",
    "os.makedirs(EPOCHS_DIR, exist_ok=True)\n",
    "os.makedirs(EVOKED_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# giving some bounds \n",
    "reject = dict(mag=4e-12, grad=4000e-13) # 4k fT and 4k ft/cm\n",
    "\n",
    "# Neutral stimulus codes (fix for left/rigth if matters, leave as is if not)\n",
    "STIM_CODES = {1, 3}\n",
    "\n",
    "# PAS response triggers → PAS labels (PAS-4 may be missing per subject)\n",
    "PAS_CODE_TO_LABEL = {10: 1, 12: 2, 14: 3, 16: 4}\n",
    "PAS_LABELS = [1, 2, 3, 4]\n",
    "\n",
    "# Stimulus-locked window/baseline\n",
    "tmin, tmax = -0.2, 0.75\n",
    "baseline   = (None, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD IN RAWS + FIND EVENTS\n",
    "\n",
    "participant_data, participant_events = {}, {}\n",
    "for f in sorted(os.listdir(GET_DIR)):\n",
    "    if f.endswith(\"_cleaned_raw.fif\"):\n",
    "        pid = f.split(\"_cleaned_raw.fif\")[0]\n",
    "        raw = mne.io.read_raw_fif(os.path.join(GET_DIR, f), preload=True)\n",
    "        participant_data[pid] = raw\n",
    "\n",
    "        ev = mne.find_events(raw, stim_channel=\"STI101\", shortest_event=1, min_duration=0.002)\n",
    "        ev = ev[ev[:, 0] >= raw.time_as_index(DRIFT_SEC)[0]]\n",
    "        participant_events[pid] = ev\n",
    "        mne.write_events(f\"{EVENTS_DIR}/{pid}-eve.fif\", ev, overwrite=True)\n",
    "\n",
    "print(\"Participants:\", list(participant_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38604a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTION:\n",
    "# assigning each stimulus the next PAS presented in that trial -> associates each brain activity with a specific PAS score, so we can plot them \n",
    "\n",
    "# returns: stimulus-only events and a metadata df with a PAS column.\n",
    "def label_stim_with_pas(events, pas_map):\n",
    "    stim_idx = np.where(np.isin(events[:, 2], list(STIM_CODES)))[0]\n",
    "    pas_idx  = np.where(np.isin(events[:, 2], list(pas_map.keys())))[0]\n",
    "\n",
    "    stim_pas = []\n",
    "    for si in stim_idx:\n",
    "        nxt = pas_idx[pas_idx > si]\n",
    "        if len(nxt) == 0:\n",
    "            stim_pas.append(np.nan)\n",
    "        else:\n",
    "            stim_pas.append(pas_map.get(events[nxt[0], 2], np.nan))\n",
    "\n",
    "    stim_events = events[stim_idx]\n",
    "    meta = pd.DataFrame({\"PAS\": stim_pas})\n",
    "    return stim_events, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING EVENTS \n",
    "\n",
    "first_pid = list(participant_events.keys())[0]\n",
    "ev = participant_events[first_pid]\n",
    "fig = mne.viz.plot_events(ev, sfreq=participant_data[first_pid].info['sfreq'],\n",
    "                          first_samp=participant_data[first_pid].first_samp)\n",
    "plt.title(f'Events - {first_pid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04744c36",
   "metadata": {},
   "source": [
    "## 4. Creating epochs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EPOCHING + ARTEFACT REJECTION\n",
    "\n",
    "\n",
    "# epochs will include PAS metadata from here\n",
    "\n",
    "participant_epochs_clean = {}\n",
    "\n",
    "for pid, raw in participant_data.items():\n",
    "    events  = participant_events[pid]\n",
    "    present = set(np.unique(events[:, 2]))\n",
    "\n",
    "    if not (present & STIM_CODES):\n",
    "        print(f\"{pid}: no stimulus codes → skip\")\n",
    "        continue\n",
    "\n",
    "    # per-subject PAS map (this handles missing PAS-4 so we don't get errors!)\n",
    "    pas_map = {c: PAS_CODE_TO_LABEL[c] for c in PAS_CODE_TO_LABEL if c in present}\n",
    "\n",
    "    stim_events, metadata = label_stim_with_pas(events, pas_map)\n",
    "    if len(stim_events) == 0:\n",
    "        print(f\"{pid}: no stimulus events after filter → skip\")\n",
    "        continue\n",
    "\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        stim_events,\n",
    "        event_id={\"stimulus_code1\": 1, \"stimulus_code3\": 3},  # change labels here if we know which was left and right, otherwise keep this way:)\n",
    "        tmin=tmin, tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        preload=True,\n",
    "        reject_by_annotation=True,\n",
    "        on_missing=\"ignore\",\n",
    "    )\n",
    "\n",
    "    # attaching PAS labels and keeping only trials with a PAS\n",
    "    epochs.metadata = metadata\n",
    "    epochs = epochs[epochs.metadata[\"PAS\"].notna().values]\n",
    "\n",
    "    # epoch-level p2p rejection\n",
    "    epochs_clean = epochs.copy()\n",
    "    epochs_clean.drop_bad(reject=reject) # e defined reject in setup chunk\n",
    "\n",
    "    # saving stuff\n",
    "    epochs_clean.save(f\"{EPOCHS_DIR}/{pid}-epo_stim_withPAS_clean.fif\", overwrite=True) # naming is essentially: epoch for stimuli with PAS score added and cleaned\n",
    "    participant_epochs_clean[pid] = epochs_clean\n",
    "\n",
    "    print(f\"{pid}: kept {len(epochs_clean)}/{len(ep)} PAS-labeled epochs after p2p rejection\")\n",
    "    print(\"    PAS counts:\", epochs_clean.metadata[\"PAS\"].value_counts(dropna=False).sort_index().to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PAS scale double check \n",
    "\n",
    "\n",
    "# having a look at how many epochs were dropped and the distribution of the PAS ratings\n",
    "\n",
    "rows = []\n",
    "for pid, ep in participant_epochs_clean.items():\n",
    "    counts = ep.metadata[\"PAS\"].value_counts().sort_index().to_dict()\n",
    "    rows.append({\"pid\": pid, **{f\"PAS{int(k)}\": int(v) for k, v in counts.items()}})\n",
    "\n",
    "pas_counts_df = pd.DataFrame(rows).fillna(0).astype({\"pid\": str})\n",
    "print(pas_counts_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is also for the report for epochs \n",
    "EPOCH_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for f in sorted(glob.glob(os.path.join(EPOCH_DIR, \"*_clean.fif\"))):\n",
    "    epochs = mne.read_epochs(f, preload=False)\n",
    "    pid = os.path.basename(f).split(\"-epo\")[0]\n",
    "\n",
    "    kept = len(epochs)\n",
    "    \n",
    "    drop_log = epochs.drop_log\n",
    "    dropped = sum([1 for d in drop_log if len(d) > 0])\n",
    "    \n",
    "    # ejection percentage \n",
    "    total = kept + dropped\n",
    "    rej_pct = 100 * dropped / total if total > 0 else np.nan\n",
    "\n",
    "    rows.append({\"pid\": pid, \"kept\": kept, \"dropped\": dropped, \"total\": total, \"rej_pct\": rej_pct})\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating df from the stats above \n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nMean rejection rate: \", df[\"rej_pct\"].mean())\n",
    "print(\"SD rejection rate:   \", df[\"rej_pct\"].std(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1a0e5",
   "metadata": {},
   "source": [
    "## 5. Computing evokeds\n",
    "\n",
    "Here, we will \n",
    "1. Compute evokeds\n",
    "2. plot per-participant overlay plots\n",
    "3. plot group overlay \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "EVOKED_DIR  = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/evokeds/pas_1_4\"\n",
    "os.makedirs(EVOKED_DIR, exist_ok=True)\n",
    "\n",
    "participant_evokeds_by_pas = {}  # {pid: {\"PAS1\": path, \"PAS2\": path, \"PAS3\": path, \"PAS4\": path}}\n",
    "\n",
    "for fif_file in sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\"))):\n",
    "    pid = os.path.basename(fif_file).split(\"-epo\")[0]\n",
    "    epochs = mne.read_epochs(fif_file, preload=True, verbose=False)\n",
    "    if epochs.metadata is None or \"PAS\" not in epochs.metadata.columns:\n",
    "        print(f\"→ {pid}: no PAS metadata; skipping.\")\n",
    "        continue\n",
    "\n",
    "    pas_vals = pd.to_numeric(epochs.metadata[\"PAS\"], errors=\"coerce\")\n",
    "    masks = {f\"PAS{i}\": (pas_vals == i).to_numpy() for i in (1,2,3,4)}\n",
    "\n",
    "    ev_paths = {}\n",
    "    for label, mask in masks.items():\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        sel = epochs[mask]\n",
    "        if len(sel) == 0:\n",
    "            continue\n",
    "        ev = sel.average()\n",
    "        ev.comment = label\n",
    "        out_f = os.path.join(EVOKED_DIR, f\"{pid}-{label}-ave.fif\")\n",
    "        ev.save(out_f, overwrite=True)\n",
    "        ev_paths[label] = out_f\n",
    "        print(f\"✓ {pid}: saved {label} → {out_f}\")\n",
    "\n",
    "    participant_evokeds_by_pas[pid] = ev_paths\n",
    "\n",
    "print(\"\\nSaved per-PAS evokeds to:\", EVOKED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LABELS = [\"PAS1\", \"PAS2\", \"PAS3\", \"PAS4\"]\n",
    "\n",
    "ga_by_label = {}\n",
    "n_subj_by_label = {}\n",
    "\n",
    "for lab in LABELS:\n",
    "    ev_list = []\n",
    "    for pid, mapping in participant_evokeds_by_pas.items():\n",
    "        path = mapping.get(lab)\n",
    "        if not path:\n",
    "            continue\n",
    "        ev = mne.read_evokeds(path, condition=0, verbose=False)\n",
    "        ev.pick_types(meg=True, eeg=False, exclude=[]) \n",
    "        ev_list.append(ev)\n",
    "\n",
    "    print(f\"{lab}: collected {len(ev_list)} evokeds\")\n",
    "    if not ev_list:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        mne.channels.equalize_channels(ev_list)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {lab}: equalize_channels failed: {e} — skipping GA.\")\n",
    "        continue\n",
    "\n",
    "    # ensure identical time base\n",
    "    t0 = ev_list[0].times\n",
    "    if not all(np.array_equal(t0, e.times) for e in ev_list):\n",
    "        print(f\"[WARN] {lab}: time vectors differ — skipping GA.\")\n",
    "        continue\n",
    "\n",
    "    ga = mne.grand_average(ev_list, interpolate_bads=True)\n",
    "    ga_by_label[lab] = ga\n",
    "    n_subj_by_label[lab] = len(ev_list)\n",
    "\n",
    "# plotting\n",
    "out_dir = f\"{EVOKED_DIR}/figs\"; os.makedirs(out_dir, exist_ok=True)\n",
    "plt.figure(figsize=(8,4))\n",
    "plotted = False\n",
    "\n",
    "for lab in LABELS:\n",
    "    ga = ga_by_label.get(lab)\n",
    "    if ga is None:\n",
    "        continue\n",
    "    data = ga.get_data()\n",
    "    if data.size == 0:\n",
    "        print(f\"[WARN] {lab}: empty GA after picks.\")\n",
    "        continue\n",
    "    gfp = np.sqrt((data**2).mean(axis=0))\n",
    "    plt.plot(ga.times, gfp, label=f\"{lab} (N={n_subj_by_label[lab]})\")\n",
    "    plotted = True\n",
    "\n",
    "plt.axvline(0, ls=\"--\", color=\"k\", lw=1)\n",
    "plt.xlabel(\"Time (s)\"); plt.ylabel(\"Global Field Power (a.u.)\")\n",
    "plt.title(\"Group — stimulus-locked GFP (PAS1–PAS4)\")\n",
    "plt.legend(frameon=False); plt.tight_layout()\n",
    "\n",
    "png = os.path.join(out_dir, \"GROUP_evoked_PAS1_4_overlay.png\")\n",
    "plt.savefig(png, dpi=150); plt.close()\n",
    "print(\"Saved group figure:\", png, \"| drew lines:\", plotted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e9701",
   "metadata": {},
   "source": [
    "#### Following group discussion: collapsing evokeds PAS 3 and 4 \n",
    "Note: the 4-level PAS scale will be kept for the plots and to give a good background for why we do the collapsing\n",
    "Reasoning:\n",
    "- scarcity of PAS 4\n",
    "- baseline issues when plotting evokeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making collapsed epoch \n",
    "\n",
    "EPOCHS_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "EVOKED_COLLAPSED_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/evokeds/collapsed_evk\"\n",
    "os.makedirs(EVOKED_COLLAPSED_DIR, exist_ok=True)\n",
    "\n",
    "participant_evokeds_by_pas = {}  # {pid: {\"PAS1\": path, \"PAS2\": path, \"PAS3_4\": path}}\n",
    "\n",
    "# oading participants \n",
    "epoch_files = sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\")))\n",
    "\n",
    "for fif_file in epoch_files:\n",
    "    pid = os.path.basename(fif_file).split(\"-epo\")[0]\n",
    "\n",
    "    # loading epochs\n",
    "    epochs = mne.read_epochs(fif_file, preload=True, verbose=False)\n",
    "    if epochs.metadata is None or \"PAS\" not in epochs.metadata.columns:\n",
    "        print(f\"→ {pid}: no PAS metadata, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # collapse PAS at epoch level: 1, 2, (3 or 4 → 3_4)\n",
    "    pas_vals = epochs.metadata[\"PAS\"].astype(float)\n",
    "    masks = {\n",
    "        \"PAS1\":  pas_vals == 1,\n",
    "        \"PAS2\":  pas_vals == 2,\n",
    "        \"PAS3_4\": pas_vals.isin([3, 4]),\n",
    "    }\n",
    "\n",
    "    ev_paths = {}\n",
    "    for label, mask in masks.items():\n",
    "        mask = mask.to_numpy()\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        sel = epochs[mask]\n",
    "        if len(sel) == 0:\n",
    "            continue\n",
    "        ev = sel.average()\n",
    "        ev.comment = label\n",
    "        out_f = os.path.join(EVOKED_COLLAPSED_DIR, f\"{pid}-{label}-ave.fif\")\n",
    "        ev.save(out_f, overwrite=True)\n",
    "        ev_paths[label] = out_f\n",
    "        print(f\"{pid}: saved {label} → {out_f}\")\n",
    "\n",
    "    if not ev_paths:\n",
    "        print(f\"→ {pid}: no PAS1/2/3_4 trials found after collapsing.\")\n",
    "    participant_evokeds_by_pas[pid] = ev_paths\n",
    "\n",
    "print(\"\\nSaved collapsed evokeds to:\", EVOKED_COLLAPSED_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Plotting per-participant GFP overlays\n",
    "\n",
    "out_dir = f\"{EVOKED_COLLAPSED_DIR}/figs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for pid, mapping in participant_evokeds_by_pas.items():\n",
    "    if not mapping:\n",
    "        print(f\"{pid}: no evokeds — skipping.\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for pas in sorted(mapping.keys()):\n",
    "        ev = mne.read_evokeds(mapping[pas], condition=0, verbose=False).pick_types(meg=True)\n",
    "        data = ev.get_data()\n",
    "        gfp  = np.sqrt((data**2).mean(axis=0))\n",
    "        plt.plot(ev.times, gfp, label=f\"PAS {pas}\")\n",
    "\n",
    "    plt.axvline(0, ls=\"--\", color=\"k\", lw=1)\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Global Field Power (a.u.)\")\n",
    "    plt.title(f\"{pid} — stimulus-locked GFP by PAS\")\n",
    "    plt.legend(frameon=False); plt.tight_layout()\n",
    "\n",
    "    out_png = os.path.join(out_dir, f\"{pid}_evoked_PAS_overlay.png\")\n",
    "    plt.savefig(out_png, dpi=150); plt.close()\n",
    "    print(\"Saved:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48550c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LABELS = [\"PAS1\", \"PAS2\", \"PAS3_4\"]\n",
    "\n",
    "ga_by_label = {}\n",
    "n_subj_by_label = {}\n",
    "\n",
    "for lab in LABELS:\n",
    "    ev_list = []\n",
    "    for pid, mapping in participant_evokeds_by_pas.items():\n",
    "        path = mapping.get(lab)\n",
    "        if not path:\n",
    "            continue\n",
    "        ev = mne.read_evokeds(path, condition=0, verbose=False)\n",
    "        ev.pick_types(meg=True, eeg=False, exclude=[])\n",
    "        ev_list.append(ev)\n",
    "\n",
    "    if not ev_list:\n",
    "        continue\n",
    "\n",
    "    mne.channels.equalize_channels(ev_list)\n",
    "\n",
    "    # ensuring identical timebase\n",
    "    base_t = ev_list[0].times\n",
    "    if not all(np.array_equal(base_t, e.times) for e in ev_list):\n",
    "        print(f\"[WARN] {lab}: time vectors differ across subjects—skipping GA for this label.\")\n",
    "        continue\n",
    "\n",
    "    ga = mne.grand_average(ev_list, interpolate_bads=True)\n",
    "    ga_by_label[lab] = ga\n",
    "    n_subj_by_label[lab] = len(ev_list)\n",
    "\n",
    "# plotting gfp\n",
    "out_dir = f\"{EVOKED_COLLAPSED_DIR}/figs\"; os.makedirs(out_dir, exist_ok=True)\n",
    "plt.figure(figsize=(8,4))\n",
    "plotted = False\n",
    "\n",
    "for lab, ga in ga_by_label.items():\n",
    "    data = ga.get_data()\n",
    "    if data.size == 0:\n",
    "        print(f\"[WARN] {lab}: GA has zero data after picks—skipping.\")\n",
    "        continue\n",
    "    gfp = np.sqrt((data**2).mean(axis=0))\n",
    "    plt.plot(ga.times, gfp, label=f\"{lab} (N={n_subj_by_label[lab]})\")\n",
    "    plotted = True\n",
    "\n",
    "plt.axvline(0, ls=\"--\", color=\"k\", lw=1)\n",
    "plt.xlabel(\"Time (s)\"); plt.ylabel(\"Global Field Power (a.u.)\")\n",
    "plt.title(\"Group — stimulus-locked GFP (PAS1, PAS2, PAS3_4)\")\n",
    "plt.legend(frameon=False); plt.tight_layout()\n",
    "\n",
    "png = os.path.join(out_dir, \"GROUP_evoked_PAS_overlay.png\")\n",
    "plt.savefig(png, dpi=150); plt.close()\n",
    "print(\"Saved group figure:\", png, \"| drew lines:\", plotted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal participants loaded: {len(participant_data)}\")\n",
    "print(\"Participants:\", list(participant_data.keys()))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Epoch counts after artifact rejection (per PAS):\")\n",
    "print(\"-\" * 60)\n",
    "for pid, ep in participant_epochs_clean.items():\n",
    "    counts = ep.metadata[\"PAS\"].value_counts().sort_index().to_dict()\n",
    "    print(f\"{pid}: total {len(ep)} epochs → {counts}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Evoked files saved to:\", EVOKED_DIR)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nPreprocessing complete! Ready for source or decoding analysis.\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "1. **Noise covariance estimation** for each participant\n",
    "2. **Forward model computation** (requires MRI/BEM)\n",
    "3. **Inverse solution** (source reconstruction)\n",
    "4. **Group-level analysis**\n",
    "\n",
    "See \"multi_participant_analysis.ipynb\" for analysis process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
