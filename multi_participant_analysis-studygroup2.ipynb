{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93dfbd0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "1. **Noise covariance estimation** for each participant\n",
    "2. **Forward model computation** (requires MRI/BEM)\n",
    "3. **Inverse solution** (source reconstruction)\n",
    "4. **Group-level analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "\n",
    "%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca99bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os, re, glob, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "from mne.minimum_norm import apply_inverse, make_inverse_operator, write_inverse_operator\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.inspection import permutation_importance\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b12e5d",
   "metadata": {},
   "source": [
    "## Noise Covariance Estimation\n",
    "\n",
    "Compute noise covariance for whitening the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing nosie cov for all participants\n",
    "\n",
    "\n",
    "SAVE_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds\"\n",
    "EPOCHS_DIR = f\"{SAVE_DIR}/epochs\"\n",
    "COV_DIR    = f\"{SAVE_DIR}/covariance\"\n",
    "\n",
    "os.makedirs(COV_DIR, exist_ok=True)\n",
    "\n",
    "# loading in epochs \n",
    "epoch_files = [f for f in os.listdir(EPOCHS_DIR) if f.endswith('-epo_stim_withPAS_clean.fif')]\n",
    "\n",
    "participant_noise_cov = {}\n",
    "\n",
    "for fname in epoch_files:\n",
    "    participant_id = fname.split('-epo')[0]\n",
    "    print(f\"\\n=== Computing noise covariance for {participant_id} ===\")\n",
    "    epochs_path = os.path.join(EPOCHS_DIR, fname)\n",
    "    epochs = mne.read_epochs(epochs_path, preload=True)\n",
    "\n",
    "    # poise cov from baseline\n",
    "    noise_cov = mne.compute_covariance(\n",
    "        epochs,\n",
    "        tmin=None,  \n",
    "        tmax=0.0,   \n",
    "        method='auto',   \n",
    "        rank='info', \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # saving\n",
    "    cov_fname = os.path.join(COV_DIR, f\"{participant_id}-cov.fif\")\n",
    "    mne.write_cov(cov_fname, noise_cov)\n",
    "\n",
    "    # storing in dict\n",
    "    participant_noise_cov[participant_id] = noise_cov\n",
    "    print(f\"✓ Done computing and saving noise covariance for {participant_id}\")\n",
    "\n",
    "print(\"\\nAll participant noise covariance matrices have been computed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting noise cov for first participant \n",
    "\n",
    "\n",
    "\n",
    "SAVE_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds\"\n",
    "EPOCHS_DIR = f\"{SAVE_DIR}/epochs\"\n",
    "COV_DIR    = f\"{SAVE_DIR}/covariance\"\n",
    "\n",
    "first_pid = list(participant_noise_cov.keys())[0]\n",
    "print(f\"\\n=== Plotting noise covariance for {first_pid} ===\")\n",
    "\n",
    "epochs_path = os.path.join(EPOCHS_DIR, f\"{first_pid}-epo_stim_withPAS_clean.fif\")\n",
    "epochs = mne.read_epochs(epochs_path, preload=False)\n",
    "\n",
    "# noise cov\n",
    "noise_cov = participant_noise_cov[first_pid]\n",
    "\n",
    "# plottingg\n",
    "fig = noise_cov.plot(epochs.info)\n",
    "plt.suptitle(f'Noise Covariance – {first_pid}', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc19892",
   "metadata": {},
   "source": [
    "## Forward model\n",
    "Computing from BEM solution (already available) and trans files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea91ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing forward solution\n",
    "\n",
    "\n",
    "SAVE_DIR    = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "EPOCHS_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "FWD_DIR     = f\"{SAVE_DIR}/forward\"\n",
    "os.makedirs(FWD_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "SUBJECTS_DIR = \"/work/freesurfer\"                   \n",
    "TRANS_ROOT   = \"/work/MEG_data/workshop_data\"        \n",
    "\n",
    "\n",
    "# helpers\n",
    "def find_trans(pid):\n",
    "    hits = glob.glob(os.path.join(TRANS_ROOT, pid, \"**\", \"workshop_2025-trans.fif\"), recursive=True)\n",
    "    if not hits:\n",
    "        hits = glob.glob(os.path.join(TRANS_ROOT, pid, \"**\", \"*trans.fif\"), recursive=True)\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def bem_path(pid):\n",
    "    return os.path.join(SUBJECTS_DIR, pid, \"bem\", f\"{pid}-5120-bem-sol.fif\")\n",
    "\n",
    "# batch\n",
    "epoch_files = sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\")))\n",
    "print(f\"Found {len(epoch_files)} epoch files in {EPOCHS_DIR}\")\n",
    "\n",
    "for ep_path in epoch_files:\n",
    "    pid = os.path.basename(ep_path).split(\"-epo\")[0]\n",
    "    print(f\"\\n→ {pid}\")\n",
    "\n",
    "    trans = find_trans(pid)\n",
    "    bem_fif = bem_path(pid)\n",
    "    if not trans or not os.path.exists(bem_fif):\n",
    "        print(f\"  Skipping (missing trans or BEM). trans_found={bool(trans)}, bem_found={os.path.exists(bem_fif)}\")\n",
    "        continue\n",
    "\n",
    "    epochs  = mne.read_epochs(ep_path, preload=False)\n",
    "    bem_sol = mne.read_bem_solution(bem_fif)\n",
    "\n",
    "    src = mne.setup_source_space(pid, spacing=\"oct6\", subjects_dir=SUBJECTS_DIR, add_dist=False, verbose=False)\n",
    "    fwd = mne.make_forward_solution(epochs.info, trans=trans, src=src, bem=bem_sol,\n",
    "                                    meg=True, eeg=False, mindist=5.0, verbose=False)\n",
    "    fwd = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True, use_cps=True)\n",
    "\n",
    "    out = os.path.join(FWD_DIR, f\"{pid}-fwd.fif\")\n",
    "    mne.write_forward_solution(out, fwd, overwrite=True)\n",
    "    print(f\"  ✓ saved {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf28707",
   "metadata": {},
   "source": [
    "## Inverse operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building inverse operator\n",
    "\n",
    "EEE_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds\"\n",
    "SAVE_DIR  = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "\n",
    "EPOCHS_DIR = f\"{EEE_DIR}/epochs\"\n",
    "COV_DIR    = f\"{EEE_DIR}/covariance\"\n",
    "FWD_DIR    = f\"{SAVE_DIR}/forward\"\n",
    "INV_DIR    = f\"{SAVE_DIR}/inverse\"\n",
    "os.makedirs(INV_DIR, exist_ok=True)\n",
    "\n",
    "epoch_files = mne.read_epochs(\"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs/0163-epo_stim_withPAS_clean.fif\")\n",
    "\n",
    "# inverse settings\n",
    "loose = 0.0          # fixed orientation - matches fwd\n",
    "\n",
    "\n",
    "for ep_path in sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\"))):\n",
    "    pid = os.path.basename(ep_path).split(\"-epo\")[0]\n",
    "    fwd_path = os.path.join(FWD_DIR, f\"{pid}-fwd.fif\")\n",
    "    cov_path = os.path.join(COV_DIR, f\"{pid}-cov.fif\")\n",
    "    if not (os.path.exists(fwd_path) and os.path.exists(cov_path)):\n",
    "        print(f\"→ {pid}: missing fwd/cov, skipping.\"); continue\n",
    "\n",
    "    epochs    = mne.read_epochs(ep_path, preload=False)\n",
    "    fwd       = mne.read_forward_solution(fwd_path)\n",
    "    noise_cov = mne.read_cov(cov_path)\n",
    "\n",
    "    inv = make_inverse_operator(\n",
    "        info=epochs.info, forward=fwd, noise_cov=noise_cov,\n",
    "        loose=loose, depth=None, rank=\"info\"\n",
    "    )\n",
    "    out = os.path.join(INV_DIR, f\"{pid}-inv.fif\")\n",
    "    write_inverse_operator(out, inv, overwrite=True)\n",
    "    print(f\"✓ {pid}: inverse → {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588614c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying inv\n",
    "\n",
    "STC_DIR= f\"{SAVE_DIR}/stc\"\n",
    "os.makedirs(STC_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "method  = \"dSPM\"\n",
    "lambda2 = 1.0 / 9.0\n",
    "\n",
    "for ep_path in sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\"))):\n",
    "    pid = os.path.basename(ep_path).split(\"-epo\")[0]\n",
    "    inv_path = os.path.join(INV_DIR, f\"{pid}-inv.fif\")\n",
    "    if not os.path.exists(inv_path):\n",
    "        print(f\"→ {pid}: missing inverse; run the inverse build chunk first.\"); continue\n",
    "\n",
    "    epochs = mne.read_epochs(ep_path, preload=False)\n",
    "    if epochs.metadata is None or \"PAS\" not in epochs.metadata.columns:\n",
    "        print(f\"→ {pid}: no PAS metadata; skipping.\"); continue\n",
    "\n",
    "    # collapsing PAS labels: 1→\"1\", 2→\"2\", 3/4→\"3_4\"\n",
    "    pas_raw = epochs.metadata[\"PAS\"].astype(float)\n",
    "    pas_collapsed = pas_raw.map(lambda x: \"1\" if x==1 else \"2\" if x==2 else \"3_4\" if x in (3,4) else np.nan)\n",
    "    keep = pas_collapsed.notna().to_numpy()\n",
    "    if keep.sum() == 0:\n",
    "        print(f\"→ {pid}: no epochs with PAS 1/2/3_4 after collapsing; skipping.\"); continue\n",
    "\n",
    "    epochs = epochs[keep]\n",
    "    pas_collapsed = pas_collapsed[keep].reset_index(drop=True)\n",
    "\n",
    "    inv = mne.minimum_norm.read_inverse_operator(inv_path)\n",
    "\n",
    "    # trial-wise source estimates\n",
    "    stcs = mne.minimum_norm.apply_inverse_epochs(epochs, inv, lambda2=lambda2, method=method,\n",
    "                                return_generator=False, verbose=False)\n",
    "\n",
    "    # one file per trial \n",
    "    out_dir = os.path.join(STC_DIR, pid)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for i, (stc_tr, lbl) in enumerate(zip(stcs, pas_collapsed)):\n",
    "        stc_tr.save(os.path.join(out_dir, f\"{pid}-trial{i+1:04d}-PAS{lbl}-{method}.h5\"), overwrite=True)\n",
    "\n",
    "    print(f\"✓ {pid}: saved {len(stcs)} trial STCs → {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebfcc8",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitening check - how is the noise covariance and the rank? (with plot)\n",
    "\n",
    "pid        = \"0164\"\n",
    "EPOCHS_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "COV_DIR    = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/covariance\"\n",
    "\n",
    "epochs    = mne.read_epochs(os.path.join(EPOCHS_DIR, f\"{pid}-epo_stim_withPAS_clean.fif\"), preload=False)\n",
    "noise_cov = mne.read_cov(os.path.join(COV_DIR, f\"{pid}-cov.fif\"))\n",
    "\n",
    "# pickng any cond in epochs \n",
    "cond = list(epochs.event_id.keys())[0]\n",
    "evk  = epochs[cond].average()\n",
    "\n",
    "# whitening plot - want flat traces around 0 in baseline, GFP near dashed line\n",
    "evk.plot_white(noise_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69193764",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SC on collapsed PAS evokeds \n",
    "\n",
    "pid = \"0163\"\n",
    "INV_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files/inverse\"\n",
    "EVK_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/evokeds/collapsed_evk\"\n",
    "COV_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/covariance\"\n",
    "\n",
    "inv  = mne.minimum_norm.read_inverse_operator(f\"{INV_DIR}/{pid}-inv.fif\")\n",
    "\n",
    "\n",
    "for name in [f\"{pid}-PAS2-ave.fif\", f\"{pid}-PAS1-ave.fif\", f\"{pid}-PAS3_4-ave.fif\"]:\n",
    "    p = os.path.join(EVK_DIR, name)\n",
    "    if os.path.exists(p):\n",
    "        evk = mne.read_evokeds(p, condition=0)\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No collapsed PAS evoked found.\")\n",
    "\n",
    "# whitening check (just numeric)\n",
    "noise_cov = mne.read_cov(os.path.join(COV_DIR, f\"{pid}-cov.fif\"))\n",
    "evk.plot_white(noise_cov)\n",
    "\n",
    "# Inverse\n",
    "stc = mne.minimum_norm.apply_inverse(evk, inv, lambda2=1/9, method=\"dSPM\")\n",
    "try:\n",
    "    vtx, t_peak, amp = stc.get_peak(hemi=\"lh\", return_amplitude=True)\n",
    "except TypeError:\n",
    "    vtx, t_peak = stc.get_peak(hemi=\"lh\")\n",
    "    ti = int(np.argmin(np.abs(stc.times - t_peak)))\n",
    "    li = int(np.where(stc.vertices[0] == vtx)[0][0])\n",
    "    amp = float(stc.data[li, ti])\n",
    "\n",
    "t = stc.times\n",
    "base = t < 0\n",
    "resp = (t >= max(t[0], t_peak-0.03)) & (t <= min(t[-1], t_peak+0.03))\n",
    "src_ratio = np.nanpercentile(np.abs(stc.data[:, resp]), 95) / max(np.nanpercentile(np.abs(stc.data[:, base]), 95), 1e-12)\n",
    "gfp_ratio = (np.mean(np.sqrt((evk.data[:, resp]**2).sum(axis=0))) /\n",
    "             max(np.mean(np.sqrt((evk.data[:, base]**2).sum(axis=0))), 1e-12))\n",
    "\n",
    "print(f\"dSPM peak: {amp:.2f} at {t_peak*1000:.0f} ms | Source ratio: {src_ratio:.2f} | GFP ratio: {gfp_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda753d",
   "metadata": {},
   "source": [
    "# Last step: Modeling subjective experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621a646",
   "metadata": {},
   "source": [
    "## Model #1: multinomial logistic regression classifier trained on predefined time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and ROI loading \n",
    "\n",
    "SAVE_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "STC_DIR    = f\"{SAVE_DIR}/stc\"\n",
    "INV_DIR    = f\"{SAVE_DIR}/inverse\"\n",
    "FEAT_DIR   = f\"{SAVE_DIR}/features\"\n",
    "EPOCHS_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "subjects_dir = \"/work/freesurfer\"\n",
    "\n",
    "os.makedirs(FEAT_DIR, exist_ok=True)\n",
    "\n",
    "# ROI groups (same as Model 3)\n",
    "OT = {\n",
    "    'pericalcarine','cuneus','lateraloccipital','lingual',\n",
    "    'fusiform','inferiortemporal','middletemporal','superiortemporal'\n",
    "}\n",
    "\n",
    "FP = {\n",
    "    'superiorfrontal','rostralmiddlefrontal','caudalmiddlefrontal',\n",
    "    'lateralorbitofrontal','medialorbitofrontal',\n",
    "    'superiorparietal','inferiorparietal','precuneus','postcentral',\n",
    "    'rostralanteriorcingulate','isthmuscingulate'\n",
    "}\n",
    "\n",
    "# ROI loading - Desikan–Killiany, native space\n",
    "roi_labels = {}\n",
    "\n",
    "for ep_path in sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\"))):\n",
    "    pid = os.path.basename(ep_path).split(\"-epo\")[0]\n",
    "    try:\n",
    "        labs = mne.read_labels_from_annot(subject=pid, parc=\"aparc\", subjects_dir=subjects_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"→ {pid}: missing aparc\")\n",
    "        continue\n",
    "\n",
    "    # drop unknown\n",
    "    sel = [lab for lab in labs if not lab.name.lower().startswith((\"unknown\",\"corpuscallosum\"))]\n",
    "    if not sel:\n",
    "        print(f\"→ {pid}: no cortical labels found\")\n",
    "        continue\n",
    "\n",
    "    roi_labels[pid] = sel\n",
    "\n",
    "    # just for information: how many ROIs fall into OT / FP / Other?\n",
    "    base_names = [lab.name.split(\"-\")[0].lower() for lab in sel]\n",
    "    n_ot    = sum(b in OT for b in base_names)\n",
    "    n_fp    = sum(b in FP for b in base_names)\n",
    "    n_other = len(sel) - n_ot - n_fp\n",
    "\n",
    "    print(f\"✓ {pid}: cached {len(sel)} aparc labels \"\n",
    "          f\"(OT={n_ot}, FP={n_fp}, Other={n_other})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a81b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction \n",
    "\n",
    "WIN_DEFS = {\n",
    "    \"VAN\": (0.150, 0.250),   # visual awareness negativity\n",
    "    \"LP\":  (0.330, 0.550),   # late positivity\n",
    "}\n",
    "BASELINE = (-0.200, 0.000)\n",
    "\n",
    "def extract_features_for_participant(pid, do_baseline=True, save_to_disk=False):\n",
    "    # find left-hemi files\n",
    "    stc_files = sorted(glob.glob(os.path.join(STC_DIR, pid, f\"{pid}-trial*-PAS*-dSPM.h5-lh.stc\")))\n",
    "    if not stc_files:\n",
    "        raise FileNotFoundError(f\"No STCs matched for {pid} in {os.path.join(STC_DIR, pid)}\")\n",
    "\n",
    "    inv = mne.minimum_norm.read_inverse_operator(os.path.join(INV_DIR, f\"{pid}-inv.fif\"))\n",
    "    src = inv[\"src\"]\n",
    "\n",
    "    labels = roi_labels.get(pid)\n",
    "    if not labels:\n",
    "        raise RuntimeError(f\"No ROI labels cached for {pid}. Run the ROI-loading step first.\")\n",
    "\n",
    "    # time indices\n",
    "    ex = mne.read_source_estimate(stc_files[0])\n",
    "    times = ex.times\n",
    "    idx_win  = {w: np.where((times >= a) & (times <= b))[0] for w, (a,b) in WIN_DEFS.items()}\n",
    "    idx_base = np.where((times >= BASELINE[0]) & (times <= BASELINE[1]))[0]\n",
    "    if do_baseline and idx_base.size == 0:\n",
    "        raise ValueError(f\"Baseline {BASELINE} outside STC range [{times[0]:.3f},{times[-1]:.3f}]\")\n",
    "\n",
    "    def pas_to_class(fp):\n",
    "        m = re.search(r\"PAS(1|2|3_4)\", fp)\n",
    "        return {'1':0, '2':1, '3_4':2}[m.group(1)] if m else None\n",
    "\n",
    "    X, y = [], []\n",
    "    for fp in stc_files:\n",
    "        c = pas_to_class(fp)\n",
    "        if c is None:\n",
    "            continue\n",
    "        y.append(c)\n",
    "\n",
    "        stc = mne.read_source_estimate(fp)  # loads both hemis\n",
    "        ltc = mne.extract_label_time_course(stc, labels, src=src, mode='mean_flip')  # (n_rois, n_times)\n",
    "\n",
    "        if do_baseline:\n",
    "            base = ltc[:, idx_base].mean(axis=1, keepdims=True)\n",
    "            ltc  = ltc - base\n",
    "\n",
    "        # averaging within VAN and LP windows\n",
    "        feats = []\n",
    "        for w in [\"VAN\", \"LP\"]:\n",
    "            feats.append(ltc[:, idx_win[w]].mean(axis=1))\n",
    "        X.append(np.concatenate(feats))\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    feat_names = [f\"{lab.name}_{w}\" for w in [\"VAN\",\"LP\"] for lab in labels]\n",
    "\n",
    "    if save_to_disk:\n",
    "        out_path = os.path.join(FEAT_DIR, f\"{pid}_features_baselineVANLP.npz\")\n",
    "        np.savez_compressed(\n",
    "            out_path,\n",
    "            X=X, y=y,\n",
    "            feature_names=np.array(feat_names, dtype=object),\n",
    "            pid=pid,\n",
    "            windows=np.array(WIN_DEFS, dtype=object),\n",
    "            baseline=np.array(BASELINE, dtype=float)\n",
    "        )\n",
    "        print(f\"✓ {pid}: saved features → {out_path}\")\n",
    "\n",
    "    return X, y, feat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing feature extraction\n",
    "\n",
    "pids = [os.path.basename(p) for p in sorted(glob.glob(os.path.join(STC_DIR, \"*\")))]\n",
    "print(f\"Found {len(pids)} participants in {STC_DIR}.\")\n",
    "\n",
    "for pid in pids:\n",
    "    try:\n",
    "        X, y, fn = extract_features_for_participant(pid, do_baseline=True, save_to_disk=True)\n",
    "        print(f\"✓ {pid}: X={X.shape}, classes={np.bincount(y)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ {pid}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking how feature extraction and ROI are doing - also make a plot potentially!\n",
    "\n",
    "pid = \"0163\" \n",
    "\n",
    "# what files are we matching: \n",
    "pattern = os.path.join(STC_DIR, pid, f\"{pid}-trial*-PAS*-dSPM.h5-lh.stc\")\n",
    "files = sorted(glob.glob(pattern))\n",
    "print(f\"\\n[{pid}] matched {len(files)} left-hemi files. First 3:\\n\", files[:3])\n",
    "\n",
    "# oading one STC \n",
    "t0 = time.perf_counter()\n",
    "stc = mne.read_source_estimate(files[0]) \n",
    "t1 = time.perf_counter()\n",
    "print(f\"read_source_estimate time: {(t1 - t0)*1000:.1f} ms\")\n",
    "print(\"stc vertices per hemi:\", [len(v) for v in stc.vertices], \"| n_times:\", len(stc.times))\n",
    "print(\"time range:\", f\"{stc.times[0]:.3f} → {stc.times[-1]:.3f} s\")\n",
    "\n",
    "# oing baseline and 1 extraction\n",
    "inv = mne.minimum_norm.read_inverse_operator(os.path.join(INV_DIR, f\"{pid}-inv.fif\"))\n",
    "ltc = mne.extract_label_time_course(stc, roi_labels[pid], src=inv[\"src\"], mode=\"mean_flip\")\n",
    "print(\"label time course shape:\", ltc.shape)  # (n_rois, n_times)\n",
    "\n",
    "# checking baseline \n",
    "BASELINE = (-0.200, 0.000)\n",
    "idx_base = np.where((stc.times >= BASELINE[0]) & (stc.times <= BASELINE[1]))[0]\n",
    "pre_means = ltc[:, idx_base].mean(axis=1)\n",
    "print(\"baseline window samples:\", idx_base.size, \"| pre-stim means (first 5):\", np.round(pre_means[:5], 3))\n",
    "\n",
    "# running plus summary \n",
    "t0 = time.perf_counter()\n",
    "X_demo, y_demo, fn_demo = extract_features_for_participant(pid, do_baseline=True)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"\\nextract_features_for_participant time: {(t1 - t0)*1000:.1f} ms\")\n",
    "print(\"X shape:\", X_demo.shape, \"| y counts:\", np.bincount(y_demo))\n",
    "print(\"First 5 feature names:\", fn_demo[:5])\n",
    "print(\"First row (first 8 features):\", np.round(X_demo[0, :8], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d699e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding per participant + check for overfitting \n",
    "\n",
    "\n",
    "OUT_CSV   = os.path.join(FEAT_DIR, \"decoding_per_participant_VANLP_with_gap.csv\")\n",
    "CV_SPLITS = 5\n",
    "\n",
    "def make_pipe():\n",
    "    return Pipeline([\n",
    "        (\"z\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=0.1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "rows = []\n",
    "cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=7)\n",
    "\n",
    "for npz_path in sorted(glob.glob(os.path.join(FEAT_DIR, \"*_features_baselineVANLP.npz\"))):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    X, y  = data[\"X\"], data[\"y\"]\n",
    "    pid   = str(data.get(\"pid\", os.path.basename(npz_path).split(\"_features_\")[0]))\n",
    "\n",
    "    tr_accs, va_accs = [], []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        clf = make_pipe()\n",
    "        clf.fit(X[tr], y[tr])\n",
    "\n",
    "        yhat_tr = clf.predict(X[tr])\n",
    "        yhat_te = clf.predict(X[te])\n",
    "\n",
    "        tr_accs.append(balanced_accuracy_score(y[tr], yhat_tr))\n",
    "        va_accs.append(balanced_accuracy_score(y[te], yhat_te))\n",
    "\n",
    "    tr_accs = np.array(tr_accs)\n",
    "    va_accs = np.array(va_accs)\n",
    "    gap     = tr_accs - va_accs\n",
    "\n",
    "    rows.append({\n",
    "        \"pid\": pid,\n",
    "        \"n_trials\": int(X.shape[0]),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "        \"val_bal_acc_mean\": float(va_accs.mean()),\n",
    "        \"val_bal_acc_sd\":   float(va_accs.std(ddof=1)),\n",
    "        \"train_bal_acc_mean\": float(tr_accs.mean()),\n",
    "        \"gap_mean\": float(gap.mean())\n",
    "    })\n",
    "    print(f\"{pid}: val={va_accs.mean():.3f}±{va_accs.std(ddof=1):.3f} | train={tr_accs.mean():.3f} | gap={gap.mean():.3f}\")\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"pid\")\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"\\nSaved per-participant results: \", OUT_CSV)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick sum across participants \n",
    "\n",
    "## Purpose: compute group level summary of mean validation balanced accuracy and mean overfitting gap across all participants \n",
    "\n",
    "df[\"val_bal_acc_mean\"].mean(), df[\"val_bal_acc_mean\"].std()\n",
    "df[\"gap_mean\"].mean(), df[\"gap_mean\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test comparing each participant's mean balanced accuracy score against chance (0.333)\n",
    "\n",
    "from scipy.stats import ttest_1samp\n",
    "chance = 1/3\n",
    "t, p = ttest_1samp(df[\"val_bal_acc_mean\"], chance)\n",
    "print(f\"t={t:.2f}, p={p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9cc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking participants\n",
    "\n",
    "## Purpose: to rank participants based on which participants contributed most robustly to the group effect. \n",
    "\n",
    "df[\"val/train_ratio\"] = df[\"val_bal_acc_mean\"] / df[\"train_bal_acc_mean\"]\n",
    "df[\"efficiency\"] = df[\"val_bal_acc_mean\"] - df[\"gap_mean\"]  # penalises overfitting\n",
    "\n",
    "df.sort_values(\"efficiency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting group summary \n",
    "\n",
    "\n",
    "# loading data\n",
    "FEAT_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files/features\"\n",
    "CSV = os.path.join(FEAT_DIR, \"decoding_per_participant_VANLP_with_gap.csv\")\n",
    "df = pd.read_csv(CSV).sort_values(\"val_bal_acc_mean\", ascending=False)\n",
    "\n",
    "pids = df[\"pid\"].astype(str).tolist()\n",
    "chance = 1/3\n",
    "\n",
    "# stats\n",
    "val_mean, val_sd = df[\"val_bal_acc_mean\"].mean(), df[\"val_bal_acc_mean\"].std(ddof=1)\n",
    "gap_mean, gap_sd = df[\"gap_mean\"].mean(), df[\"gap_mean\"].std(ddof=1)\n",
    "t, p = ttest_1samp(df[\"val_bal_acc_mean\"], chance)\n",
    "print(f\"Group val = {val_mean:.3f} ± {val_sd:.3f}\")\n",
    "print(f\"Group gap = {gap_mean:.3f} ± {gap_sd:.3f}\")\n",
    "print(f\"t-test vs chance (0.333): t={t:.2f}, p={p:.3f}\")\n",
    "\n",
    "# 1st plot: validation vs training accuracy\n",
    "x = np.arange(len(pids))\n",
    "w = 0.38\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x - w/2, df[\"val_bal_acc_mean\"], width=w, label=\"Validation\", alpha=0.9)\n",
    "plt.bar(x + w/2, df[\"train_bal_acc_mean\"], width=w, label=\"Training\", alpha=0.5)\n",
    "plt.axhline(chance, color=\"k\", ls=\"--\", lw=1, label=\"Chance (1/3)\")\n",
    "plt.xticks(x, pids, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Balanced accuracy\")\n",
    "plt.title(f\"Per-participant decoding (VAN + LP) — t={t:.2f}, p={p:.3f}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "f1 = os.path.join(FEAT_DIR, \"group_decoding_bar_VANLP.png\")\n",
    "plt.savefig(f1, dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved:\", f1)\n",
    "\n",
    "# 2nd plot: overfitting per participant\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.bar(pids, df[\"gap_mean\"], color=\"tab:orange\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Train − Val\")\n",
    "plt.title(\"Overfitting gap per participant (sorted by val. accuracy)\")\n",
    "plt.tight_layout()\n",
    "f2 = os.path.join(FEAT_DIR, \"overfit_gap_per_participant.png\")\n",
    "plt.savefig(f2, dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved:\", f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paths\n",
    "IMP_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files/importance\"\n",
    "OUT_DIR = os.path.join(IMP_DIR, \"plots\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# loading participants\n",
    "files = sorted(glob.glob(os.path.join(IMP_DIR, \"*_features.csv\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No importance files found in {IMP_DIR}\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Helper to split \"feature\" to ROI + window\n",
    "def split_feature(f):\n",
    "    roi, win = f.rsplit(\"_\", 1)\n",
    "    return roi, win\n",
    "\n",
    "df_all[[\"roi\", \"window\"]] = df_all[\"feature\"].apply(\n",
    "    lambda x: pd.Series(split_feature(x))\n",
    ")\n",
    "\n",
    "# eeping VAN and LP only \n",
    "df_filtered = df_all[df_all[\"window\"].isin([\"VAN\", \"LP\"])]\n",
    "\n",
    "mean_imp = df_filtered.groupby([\"roi\", \"window\"])[\"importance\"].mean().unstack()\n",
    "\n",
    "# lotting heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(\n",
    "    mean_imp,\n",
    "    cmap=\"magma\",\n",
    "    cbar_kws={\"label\": \"Mean permutation importance\"},\n",
    "    annot=False\n",
    ")\n",
    "\n",
    "plt.title(\"ROI × window importance (Model 1)\")\n",
    "plt.xlabel(\"Time window\")\n",
    "plt.ylabel(\"ROI\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(OUT_DIR, \"roi_window_importance_heatmap.png\")\n",
    "plt.savefig(save_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved heatmap to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model_1\")\n",
    "IMP_DIR   = os.path.join(MODEL_DIR, \"importance\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(IMP_DIR, exist_ok=True)\n",
    "\n",
    "FEAT_DIR  = os.path.join(BASE_DIR, \"features\")\n",
    "RES_CSV   = os.path.join(FEAT_DIR, \"decoding_per_participant_VANLP_with_gap.csv\")\n",
    "\n",
    "df = pd.read_csv(RES_CSV).sort_values(\"pid\").reset_index(drop=True)\n",
    "chance = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11df72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group stats plus 95% CI - pretty much the same as the ones before, but most report-ready and pretty!\n",
    "\n",
    "mean_acc  = df[\"val_bal_acc_mean\"].mean()\n",
    "sem_acc   = df[\"val_bal_acc_mean\"].std(ddof=1)/np.sqrt(len(df))\n",
    "ci95 = (mean_acc - 1.96*sem_acc, mean_acc + 1.96*sem_acc)\n",
    "\n",
    "t, p = ttest_1samp(df[\"val_bal_acc_mean\"], chance)\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"n_participants\": len(df),\n",
    "    \"group_mean_val\": mean_acc,\n",
    "    \"ci95_low\": ci95[0],\n",
    "    \"ci95_high\": ci95[1],\n",
    "    \"t_vs_chance\": t,\n",
    "    \"p_vs_chance\": p,\n",
    "    \"group_mean_gap\": df[\"gap_mean\"].mean(),\n",
    "}])\n",
    "summary_path = os.path.join(MODEL_DIR, \"group_summary.csv\")\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"Group mean = {mean_acc:.3f} (95% CI {ci95[0]:.3f}–{ci95[1]:.3f})\")\n",
    "print(f\"t-test vs chance (0.333): t={t:.2f}, p={p:.4f}\")\n",
    "print(\"Saved:\", summary_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting measures, focusing on the gap itself\n",
    "\n",
    "# side-by-side bars\n",
    "x = np.arange(len(df))\n",
    "w = 0.38\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x - w/2, df[\"val_bal_acc_mean\"], width=w, label=\"Validation\", alpha=0.9)\n",
    "plt.bar(x + w/2, df[\"train_bal_acc_mean\"], width=w, label=\"Training\", alpha=0.5)\n",
    "plt.axhline(chance, color=\"k\", ls=\"--\", lw=1, label=\"Chance (1/3)\")\n",
    "plt.xticks(x, df[\"pid\"].astype(str), rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Balanced accuracy\")\n",
    "plt.title(\"Per-participant decoding (VAN + LP)\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "f1 = os.path.join(MODEL_DIR, \"per_participant_val_vs_train.png\")\n",
    "plt.savefig(f1, dpi=200); plt.close(); print(\"Saved:\", f1)\n",
    "\n",
    "# gap per participant + histogram\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.bar(df[\"pid\"].astype(str), df[\"gap_mean\"], color=\"tab:orange\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.ylabel(\"Train − Val\")\n",
    "plt.title(\"Overfitting gap per participant\"); plt.tight_layout()\n",
    "f2 = os.path.join(MODEL_DIR, \"overfit_gap_per_participant.png\")\n",
    "plt.savefig(f2, dpi=200); plt.close(); print(\"Saved:\", f2)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(df[\"gap_mean\"], bins=8, edgecolor=\"k\")\n",
    "plt.xlabel(\"Train − Val\"); plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of overfitting gap\"); plt.tight_layout()\n",
    "f3 = os.path.join(MODEL_DIR, \"overfit_gap_hist.png\")\n",
    "plt.savefig(f3, dpi=200); plt.close(); print(\"Saved:\", f3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee44a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation importance per feature (fitted on all trials per pid)\n",
    "\n",
    "## Purpose: answering the question: which source-space regions in VAN and LP regions drive awareness decoding most strongly? \n",
    "\n",
    "def make_pipe_for_imp():\n",
    "    return Pipeline([\n",
    "        (\"z\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(multi_class=\"multinomial\",\n",
    "                                  solver=\"lbfgs\", max_iter=2000,\n",
    "                                  class_weight=\"balanced\"))\n",
    "    ])\n",
    "\n",
    "def compute_perm_importance_for_pid(npz_path, n_repeats=50, random_state=7):\n",
    "    dat = np.load(npz_path, allow_pickle=True)\n",
    "    X, y = dat[\"X\"], dat[\"y\"]\n",
    "    fn   = dat[\"feature_names\"].astype(str)\n",
    "    pid  = str(dat.get(\"pid\", os.path.basename(npz_path).split(\"_features_\")[0]))\n",
    "\n",
    "    pipe = make_pipe_for_imp().fit(X, y)\n",
    "    res  = permutation_importance(pipe, X, y, scoring=\"balanced_accuracy\",\n",
    "                                  n_repeats=n_repeats, random_state=random_state)\n",
    "    imp  = pd.DataFrame({\n",
    "        \"pid\": pid,\n",
    "        \"feature\": fn,\n",
    "        \"importance\": res.importances_mean,\n",
    "        \"importance_sd\": res.importances_std\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    out_csv = os.path.join(IMP_DIR, f\"{pid}_perm_importance_features.csv\")\n",
    "    imp.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved feature importance → {out_csv}\")\n",
    "    return imp\n",
    "\n",
    "all_imps = []\n",
    "for npz in sorted(glob.glob(os.path.join(FEAT_DIR, \"*_features_baselineVANLP.npz\"))):\n",
    "    all_imps.append(compute_perm_importance_for_pid(npz, n_repeats=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate confusion matrix - important, keep!\n",
    "\n",
    "BASE_DIR  = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "FEAT_DIR  = os.path.join(BASE_DIR, \"features\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model_1\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "def make_pipe():\n",
    "    return Pipeline([(\"z\", StandardScaler()),\n",
    "                     (\"clf\", LogisticRegression(multi_class=\"multinomial\",\n",
    "                                               solver=\"lbfgs\", max_iter=2000,\n",
    "                                               class_weight=\"balanced\", C=0.1))])\n",
    "\n",
    "labels = [\"PAS1\",\"PAS2\",\"PAS3_4\"]\n",
    "cm = np.zeros((3,3), dtype=float)\n",
    "n_tot = 0\n",
    "\n",
    "for npz in sorted(glob.glob(os.path.join(FEAT_DIR, \"*_features_baselineVANLP.npz\"))):\n",
    "    d = np.load(npz, allow_pickle=True)\n",
    "    X, y = d[\"X\"], d[\"y\"]\n",
    "    y_pred = cross_val_predict(make_pipe(), X, y, cv=cv, method=\"predict\")\n",
    "    cm += confusion_matrix(y, y_pred, labels=[0,1,2]).astype(float)\n",
    "    n_tot += len(y)\n",
    "\n",
    "cm_norm = (cm.T / cm.sum(axis=1)).T\n",
    "plt.figure(figsize=(4.2,3.8))\n",
    "im = plt.imshow(cm_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(range(3), labels); plt.yticks(range(3), labels)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Aggregate confusion matrix (recall normalised)\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "out = os.path.join(MODEL_DIR, \"aggregate_confusion_matrix.png\")\n",
    "plt.savefig(out, dpi=200); plt.close(); print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905835fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation null significance per participant \n",
    "\n",
    "\n",
    "BASE_DIR  = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "FEAT_DIR  = os.path.join(BASE_DIR, \"features\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model_1\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# CV + model\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "def make_pipe():\n",
    "    return Pipeline([\n",
    "        (\"z\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=0.1  # keeping consistent with main Model 1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def perm_p_value(obs, null):\n",
    "    # exact permutation p with add-one smoothing\n",
    "    return (1 + np.sum(null >= obs)) / (1 + len(null))\n",
    "\n",
    "def perm_null_stats(X, y, n_perm=200, rng_seed=0):\n",
    "    pipe = make_pipe()\n",
    "    # observed (cross-validated)\n",
    "    obs = cross_val_score(pipe, X, y, cv=cv, scoring=\"balanced_accuracy\").mean()\n",
    "    # permutation null\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    null = np.empty(n_perm, dtype=float)\n",
    "    for i in range(n_perm):\n",
    "        y_perm = sk_shuffle(y, random_state=int(rng.randint(0, 1e9)))\n",
    "        null[i] = cross_val_score(pipe, X, y_perm, cv=cv, scoring=\"balanced_accuracy\").mean()\n",
    "    null_mean = float(null.mean())\n",
    "    null_sd   = float(null.std(ddof=1))\n",
    "    z         = (obs - null_mean) / null_sd if null_sd > 0 else np.nan\n",
    "    p_perm    = perm_p_value(obs, null)\n",
    "    return float(obs), null_mean, null_sd, float(z), float(p_perm)\n",
    "\n",
    "rows = []\n",
    "for npz in sorted(glob.glob(os.path.join(FEAT_DIR, \"*_features_baselineVANLP.npz\"))):\n",
    "    d   = np.load(npz, allow_pickle=True)\n",
    "    X   = d[\"X\"]; y = d[\"y\"]\n",
    "    pid = str(d.get(\"pid\", os.path.basename(npz).split(\"_features_\")[0]))\n",
    "    obs, m, s, z, p = perm_null_stats(X, y, n_perm=200, rng_seed=7)\n",
    "    rows.append({\"pid\": pid, \"observed\": obs, \"null_mean\": m, \"null_sd\": s, \"z_score\": z, \"p_perm\": p})\n",
    "    print(f\"{pid}: obs={obs:.3f}, null={m:.3f}±{s:.3f}, z={z:.2f}, p_perm={p:.4f}\")\n",
    "\n",
    "df_z = pd.DataFrame(rows).sort_values(\"pid\").reset_index(drop=True)\n",
    "\n",
    "# FDR across participants\n",
    "rej, p_fdr, _, _ = multipletests(df_z[\"p_perm\"].values, method=\"fdr_bh\")\n",
    "df_z[\"p_fdr\"] = p_fdr\n",
    "df_z[\"sig_fdr_05\"] = rej\n",
    "\n",
    "csv_path = os.path.join(MODEL_DIR, \"perm_zscores.csv\")\n",
    "df_z.to_csv(csv_path, index=False)\n",
    "print(\"✓ saved z-scores & permutation p-values →\", csv_path)\n",
    "\n",
    "# --- Plot: z-scores with significance threshold ---\n",
    "plt.figure(figsize=(6,4))\n",
    "colors = np.where(df_z[\"sig_fdr_05\"], \"tab:purple\", \"lightgray\")\n",
    "plt.bar(df_z[\"pid\"].astype(str), df_z[\"z_score\"], color=colors, alpha=0.9, edgecolor=\"k\", linewidth=0.4)\n",
    "plt.axhline(1.96, color=\"gray\", ls=\"--\", lw=1, label=\"≈ p < 0.05 (z=1.96)\")\n",
    "plt.axhline(0, color=\"black\", lw=0.8)\n",
    "plt.ylabel(\"Permutation z-score\")\n",
    "plt.title(\"Significance of decoding per participant (FDR-highlighted)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "out = os.path.join(MODEL_DIR, \"zscore_per_participant.png\")\n",
    "plt.savefig(out, dpi=200); plt.close(); print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oading per participant permutation importances\n",
    "\n",
    "IMP_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files/importance\"\n",
    "files = sorted(glob.glob(os.path.join(IMP_DIR, \"*features.csv\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No importance CSVs found in {IMP_DIR}\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ean importance for participants\n",
    "mean_imp = df_all.groupby(\"feature\")[[\"LP\", \"VAN\"]].mean()\n",
    "mean_imp = mean_imp.sort_index()\n",
    "\n",
    "# lotting heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(\n",
    "    mean_imp,\n",
    "    cmap=\"magma\",\n",
    "    annot=False,\n",
    "    cbar_kws={\"label\": \"Mean permutation importance\"}\n",
    ")\n",
    "\n",
    "plt.title(\"ROI × window importance (mean across participants)\")\n",
    "plt.xlabel(\"Time window\")\n",
    "plt.ylabel(\"ROI\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc32cc",
   "metadata": {},
   "source": [
    "## Model #2: Pairwise PAS decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "def make_pipe():\n",
    "    return Pipeline([\n",
    "        (\"z\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=2000,\n",
    "                                   class_weight=\"balanced\"))\n",
    "    ])\n",
    "\n",
    "PAIRWISE = [(0,1,\"PAS1_vs_PAS2\"), (1,2,\"PAS2_vs_PAS3_4\"), (0,2,\"PAS1_vs_PAS3_4\")]\n",
    "rows = []\n",
    "for npz_path in sorted(glob.glob(os.path.join(FEAT_DIR, \"*_features_baselineVANLP.npz\"))):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    X, y, pid = d[\"X\"], d[\"y\"], str(d.get(\"pid\", os.path.basename(npz_path).split(\"_features_\")[0]))\n",
    "    for a,b,label in PAIRWISE:\n",
    "        m = np.isin(y, [a,b]); \n",
    "        if m.sum() < 10: continue\n",
    "        Xs, ys = X[m], y[m]\n",
    "        ybin = (ys==b).astype(int)\n",
    "        acc = cross_val_score(make_pipe(), Xs, ybin, cv=cv, scoring=\"balanced_accuracy\").mean()\n",
    "        rows.append({\"pid\": pid, \"pair\": label, \"bal_acc\": acc})\n",
    "\n",
    "pair_df = pd.DataFrame(rows).sort_values([\"pair\",\"pid\"])\n",
    "pair_csv = os.path.join(MODEL_DIR, \"pairwise_results.csv\")\n",
    "pair_df.to_csv(pair_csv, index=False)\n",
    "print(\"Saved pairwise:\", pair_csv)\n",
    "\n",
    "# t-tests vs chance=0.5 for each pair\n",
    "summ_rows = []\n",
    "for p in pair_df[\"pair\"].unique():\n",
    "    vals = pair_df.loc[pair_df[\"pair\"]==p, \"bal_acc\"].values\n",
    "    t, pval = ttest_1samp(vals, 0.5)\n",
    "    summ_rows.append({\"pair\": p, \"mean\": vals.mean(), \"sd\": vals.std(ddof=1), \"t\": t, \"p\": pval})\n",
    "pair_summary = pd.DataFrame(summ_rows).sort_values(\"pair\")\n",
    "pair_summary.to_csv(os.path.join(MODEL_DIR, \"pairwise_summary.csv\"), index=False)\n",
    "print(pair_summary)\n",
    "\n",
    "# boxplot (matplotlib)\n",
    "plt.figure(figsize=(6,4))\n",
    "groups = [pair_df.loc[pair_df[\"pair\"]==p, \"bal_acc\"].values for p in pair_summary[\"pair\"]]\n",
    "plt.boxplot(groups, labels=pair_summary[\"pair\"], patch_artist=True)\n",
    "plt.axhline(0.5, color=\"gray\", ls=\"--\"); plt.ylabel(\"Balanced accuracy\")\n",
    "plt.title(\"Pairwise PAS decoding\"); plt.tight_layout()\n",
    "bp = os.path.join(MODEL_DIR, \"pairwise_boxplot.png\")\n",
    "plt.savefig(bp, dpi=200); plt.close(); print(\"Saved:\", bp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d45a71",
   "metadata": {},
   "source": [
    "## Model #3: Cumulative time decoding model based on logistic regression fitting over entire epoch timepoints\n",
    "\n",
    "Tasks: \n",
    "\n",
    "- build ROI×time data per participant (baseline + decimate)\n",
    "- run cumulative decoding (all ROIs, OT-only, FP-only)\n",
    "- save per-participant curves and a group plot with VAN/LP shading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "\n",
    "# paths\n",
    "SAVE_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files\"\n",
    "STC_DIR  = f\"{SAVE_DIR}/stc\"\n",
    "INV_DIR  = f\"{SAVE_DIR}/inverse\"\n",
    "OUT_DIR  = f\"{SAVE_DIR}/cumulative_fast\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "BASELINE = (-0.200, 0.000)\n",
    "VAN_WIN  = (0.150, 0.250)\n",
    "LP_WIN   = (0.330, 0.550)\n",
    "chance   = 1/3\n",
    "\n",
    "# runtime/sampling specs\n",
    "DECIMATE  = 4\n",
    "STEP      = 3\n",
    "CUTOFF    = 0.55\n",
    "CV_SPLITS = 5         \n",
    "\n",
    "# Regularisation/feature\n",
    "SUPER_K     = 4        # trials to average into one super-trial (train side)\n",
    "AVG_TEST    = False    # do not average test fold\n",
    "TOPN_ROI    = 40       # keeping top-N ROIs by variance in training fold\n",
    "RAND_SEED   = 13       # reproducibility\n",
    "\n",
    "# ROI groups (Desikan–Killiany) - same as model 1!\n",
    "OT = {'pericalcarine','cuneus','lateraloccipital','lingual',\n",
    "      'fusiform','inferiortemporal','middletemporal','superiortemporal'}\n",
    "FP = {'superiorfrontal','rostralmiddlefrontal','caudalmiddlefrontal',\n",
    "      'lateralorbitofrontal','medialorbitofrontal',\n",
    "      'superiorparietal','inferiorparietal','precuneus','postcentral',\n",
    "      'rostralanteriorcingulate','isthmuscingulate'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting subjects and labels\n",
    "\n",
    "subjects_dir = \"/work/freesurfer\"\n",
    "EPOCHS_DIR   = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/events_epochs_evokeds/epochs\"\n",
    "\n",
    "roi_labels = {}\n",
    "for ep_path in sorted(glob.glob(os.path.join(EPOCHS_DIR, \"*-epo_stim_withPAS_clean.fif\"))):\n",
    "    pid = os.path.basename(ep_path).split(\"-epo\")[0]\n",
    "    try:\n",
    "        labs = mne.read_labels_from_annot(subject=pid, parc=\"aparc\", subjects_dir=subjects_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"→ {pid}: missing aparc\"); continue\n",
    "    sel = [lab for lab in labs if not lab.name.lower().startswith((\"unknown\",\"corpuscallosum\"))]\n",
    "    roi_labels[pid] = sel\n",
    "    print(f\"✓ {pid}: {len(sel)} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dc11816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roi_time_data_fast(pid, labels_dict, decimate=DECIMATE):\n",
    "    stc_files = sorted(glob.glob(os.path.join(STC_DIR, pid, f\"{pid}-trial*-PAS*-dSPM.h5-lh.stc\")))\n",
    "    if not stc_files:\n",
    "        raise FileNotFoundError(f\"No STCs for {pid}\")\n",
    "\n",
    "    inv = mne.minimum_norm.read_inverse_operator(os.path.join(INV_DIR, f\"{pid}-inv.fif\"))\n",
    "    src = inv[\"src\"]\n",
    "    labels = labels_dict[pid]\n",
    "\n",
    "    ex = mne.read_source_estimate(stc_files[0])\n",
    "    times_full = ex.times\n",
    "    idx_base = np.where((times_full >= BASELINE[0]) & (times_full <= BASELINE[1]))[0]\n",
    "    idx_cut  = np.where(times_full <= CUTOFF)[0]\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for fp in stc_files:\n",
    "        m = re.search(r\"PAS(1|2|3_4)\", fp)\n",
    "        if not m: continue\n",
    "        y_list.append({'1':0,'2':1,'3_4':2}[m.group(1)])\n",
    "        stc = mne.read_source_estimate(fp)\n",
    "        ltc = mne.extract_label_time_course(stc, labels, src=src, mode='mean_flip')\n",
    "        base = ltc[:, idx_base].mean(axis=1, keepdims=True)\n",
    "        ltc  = (ltc - base)[:, idx_cut][:, ::decimate]\n",
    "        X_list.append(ltc)\n",
    "\n",
    "    X_time = np.stack(X_list, axis=0)  # (n_trials, n_rois, n_times)\n",
    "    y = np.asarray(y_list)\n",
    "    times = times_full[idx_cut][::decimate]\n",
    "    feat_names = [lab.name for lab in labels]\n",
    "    return X_time, y, times, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91640b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lda_shrink():\n",
    "    # stronger shrinkage to cut overfit\n",
    "    return Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"lda\", LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=0.5))\n",
    "    ])\n",
    "\n",
    "def features_from_window(X_time, t_index):\n",
    "    return X_time[:, :, :t_index+1].mean(axis=2)   # mean-pool time\n",
    "\n",
    "def _make_supertrials(X, y, k, random_state, avg_test=False, is_train=True):\n",
    "    if not is_train and not avg_test or k <= 1:\n",
    "        return X, y\n",
    "    rs = np.random.RandomState(random_state)\n",
    "    X_out, y_out = [], []\n",
    "    for cls in np.unique(y):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        rs.shuffle(idx)\n",
    "        if len(idx) < k:\n",
    "            X_out.append(X[idx].mean(axis=0, keepdims=True))\n",
    "            y_out.append([cls]); continue\n",
    "        n_grp = len(idx)//k\n",
    "        idx = idx[:n_grp*k].reshape(n_grp, k)\n",
    "        X_grp = X[idx].mean(axis=1)\n",
    "        y_grp = np.full(n_grp, cls)\n",
    "        X_out.append(X_grp); y_out.append(y_grp)\n",
    "    return np.vstack(X_out), np.concatenate(y_out)\n",
    "\n",
    "def _topn_roi_indices(X_train, topn):\n",
    "    if topn is None or topn <= 0 or topn >= X_train.shape[1]:\n",
    "        return None\n",
    "    var = X_train.var(axis=0)\n",
    "    return np.argsort(var)[-topn:]\n",
    "\n",
    "def cumulative_decoding_curve_overfit(X_time, y, times, roi_mask=None):\n",
    "    if roi_mask is not None:\n",
    "        X_time = X_time[:, roi_mask, :]\n",
    "\n",
    "    if len(np.unique(y)) < 2:\n",
    "        raise ValueError(\"Only one label present.\")\n",
    "    if min(np.bincount(y)) < CV_SPLITS:\n",
    "        raise ValueError(f\"Too few samples for {CV_SPLITS}-fold CV.\")\n",
    "\n",
    "    n_times = X_time.shape[2]\n",
    "    idxs = list(range(0, n_times, STEP))\n",
    "    t_sub = times[idxs]\n",
    "\n",
    "    acc_train, acc_val = np.zeros(len(idxs)), np.zeros(len(idxs))\n",
    "    cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=7)\n",
    "\n",
    "    for j, t in enumerate(idxs):\n",
    "        X_feat = features_from_window(X_time, t)\n",
    "        fold_tr, fold_va = [], []\n",
    "        for fold_id, (tr, te) in enumerate(cv.split(X_feat, y)):\n",
    "            X_tr, y_tr = X_feat[tr], y[tr]\n",
    "            X_te, y_te = X_feat[te], y[te]\n",
    "\n",
    "            # top-N ROI selection\n",
    "            roi_idx = _topn_roi_indices(X_tr, TOPN_ROI)\n",
    "            if roi_idx is not None:\n",
    "                X_tr, X_te = X_tr[:, roi_idx], X_te[:, roi_idx]\n",
    "\n",
    "            # super-trial averaging\n",
    "            X_tr_st, y_tr_st = _make_supertrials(X_tr, y_tr, k=SUPER_K,\n",
    "                random_state=RAND_SEED + j + 17*fold_id, avg_test=False, is_train=True)\n",
    "            X_te_st, y_te_st = _make_supertrials(X_te, y_te, k=SUPER_K,\n",
    "                random_state=RAND_SEED + j + 17*fold_id, avg_test=AVG_TEST, is_train=False)\n",
    "\n",
    "            clf = make_lda_shrink()\n",
    "            clf.fit(X_tr_st, y_tr_st)\n",
    "            fold_tr.append(balanced_accuracy_score(y_tr_st, clf.predict(X_tr_st)))\n",
    "            fold_va.append(balanced_accuracy_score(y_te_st, clf.predict(X_te_st)))\n",
    "\n",
    "        acc_train[j] = np.mean(fold_tr)\n",
    "        acc_val[j]   = np.mean(fold_va)\n",
    "\n",
    "    gap = acc_train - acc_val\n",
    "    return acc_train, acc_val, gap, t_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8539f",
   "metadata": {},
   "source": [
    "### NOTE: run below chunk with 64-core machine for quicker output. Otherwise change parallelisation settings to lower bounds so the kernel doesn't crash! Make sure to switch to smaller machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unning for all participants\n",
    "\n",
    "def run_participant(pid):\n",
    "    try:\n",
    "        X_time, y, times, feat_labels = load_roi_time_data_fast(pid, roi_labels)\n",
    "        base_names = [nm.split(\"-\")[0].lower() for nm in feat_labels]\n",
    "        roi_mask_ot = np.array([bn in OT for bn in base_names])\n",
    "        roi_mask_fp = np.array([bn in FP for bn in base_names])\n",
    "\n",
    "        tr_all, va_all, gap_all, t_sub = cumulative_decoding_curve_overfit(X_time, y, times)\n",
    "        tr_ot,  va_ot,  gap_ot,  _     = cumulative_decoding_curve_overfit(X_time, y, times, roi_mask_ot)\n",
    "        tr_fp,  va_fp,  gap_fp,  _     = cumulative_decoding_curve_overfit(X_time, y, times, roi_mask_fp)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"time_s\": t_sub,\n",
    "            \"train_all\": tr_all, \"val_all\": va_all, \"gap_all\": gap_all,\n",
    "            \"train_OT\": tr_ot, \"val_OT\": va_ot, \"gap_OT\": gap_ot,\n",
    "            \"train_FP\": tr_fp, \"val_FP\": va_fp, \"gap_FP\": gap_fp\n",
    "        })\n",
    "        out_csv = os.path.join(OUT_DIR, f\"{pid}_cumulative_curves_overfit.csv\")\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        return {\"pid\": pid, \"status\": \"ok\", \"n_trials\": len(y)}\n",
    "    except Exception as e:\n",
    "        return {\"pid\": pid, \"status\": \"error\", \"msg\": str(e)}\n",
    "\n",
    "pids = sorted(next(os.walk(STC_DIR))[1])\n",
    "print(f\"Running {len(pids)} participants in parallel…\")\n",
    "\n",
    "results = Parallel(n_jobs=8)(\n",
    "    delayed(run_participant)(pid) for pid in tqdm(pids)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "for r in results:\n",
    "    if r[\"status\"] == \"ok\":\n",
    "        print(f\"✓ {r['pid']} ({r['n_trials']} trials)\")\n",
    "    else:\n",
    "        print(f\"✗ {r['pid']}: {r['msg']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group mean and plot\n",
    "\n",
    "# getting participant info\n",
    "files = sorted(glob.glob(os.path.join(OUT_DIR, \"*_cumulative_curves_overfit.csv\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No overfit CSVs found in {OUT_DIR}\")\n",
    "\n",
    "curves_all_val = [pd.read_csv(f)[\"val_all\"].to_numpy() for f in files]\n",
    "curves_ot_val  = [pd.read_csv(f)[\"val_OT\"].to_numpy()  for f in files]\n",
    "curves_fp_val  = [pd.read_csv(f)[\"val_FP\"].to_numpy()  for f in files]\n",
    "times = pd.read_csv(files[0])[\"time_s\"].to_numpy()\n",
    "\n",
    "gm_all = np.vstack(curves_all_val).mean(axis=0)\n",
    "gm_ot  = np.vstack(curves_ot_val).mean(axis=0)\n",
    "gm_fp  = np.vstack(curves_fp_val).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(7.5, 4.3))\n",
    "plt.plot(times, gm_all, label=\"All ROIs (val)\", lw=2)\n",
    "plt.plot(times, gm_ot,  label=\"Occipito-temporal (val)\", lw=1.8)\n",
    "plt.plot(times, gm_fp,  label=\"Frontal–parietal (val)\", lw=1.8)\n",
    "plt.axvspan(VAN_WIN[0], VAN_WIN[1], color=\"grey\", alpha=0.15, label=\"VAN (150–250 ms)\")\n",
    "plt.axvspan(LP_WIN[0],  LP_WIN[1],  color=\"grey\", alpha=0.10, label=\"LP (330–550 ms)\")\n",
    "plt.axhline(1/3, color=\"k\", ls=\"--\", lw=0.8, label=\"Chance (3-class)\")\n",
    "plt.xlim(times[0], times[-1])\n",
    "plt.ylim(0.30, max(gm_all.max(), gm_ot.max(), gm_fp.max()) + 0.06)\n",
    "plt.xlabel(\"Time (s)\"); plt.ylabel(\"Balanced accuracy (validation)\")\n",
    "plt.title(\"Cumulative decoding over time (group mean)\")\n",
    "plt.legend(loc=\"lower right\", ncol=2, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"group_cumulative_plot_val.png\"), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# overfitting gap plot \n",
    "gm_gap = np.vstack([pd.read_csv(f)[\"gap_all\"].to_numpy() for f in files]).mean(axis=0)\n",
    "plt.figure(figsize=(7.5, 2.8))\n",
    "plt.plot(times, gm_gap, lw=2, label=\"All ROIs (mean gap)\")\n",
    "plt.axhline(0, color=\"k\", ls=\"--\", lw=0.8)\n",
    "plt.axvspan(VAN_WIN[0], VAN_WIN[1], color=\"grey\", alpha=0.15)\n",
    "plt.axvspan(LP_WIN[0],  LP_WIN[1],  color=\"grey\", alpha=0.10)\n",
    "plt.xlabel(\"Time (s)\"); plt.ylabel(\"Train − Val\"); plt.title(\"Group mean overfitting gap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"group_overfit_gap_plot.png\"), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY: nicer and more informative plot: \n",
    "\n",
    "chance = 1/3\n",
    "\n",
    "def mean_sem(arr_list):\n",
    "    M = np.vstack(arr_list)\n",
    "    m = M.mean(axis=0)\n",
    "    se = M.std(axis=0, ddof=1) / np.sqrt(M.shape[0])\n",
    "    return m, se\n",
    "\n",
    "def first_exceed(time, curve, base=chance, delta=0.02, min_dur=0.03):\n",
    "    thr = base + delta\n",
    "    ok = curve > thr\n",
    "    step = float(np.median(np.diff(time)))\n",
    "    run = 0\n",
    "    for i, flag in enumerate(ok):\n",
    "        run = run + 1 if flag else 0\n",
    "        if run * step >= min_dur:\n",
    "            return float(time[i - run + 1])\n",
    "    return np.nan\n",
    "\n",
    "def peak_in_window(time, curve, w):\n",
    "    m = (time >= w[0]) & (time <= w[1])\n",
    "    if not np.any(m):\n",
    "        return np.nan, np.nan\n",
    "    idx = np.argmax(curve[m])\n",
    "    sel = np.where(m)[0][idx]\n",
    "    return float(curve[sel]), float(time[sel])\n",
    "\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(OUT_DIR, \"*_cumulative_curves_overfit.csv\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No overfit CSVs found in {OUT_DIR}\")\n",
    "\n",
    "# loading curves\n",
    "curves_all_val = [pd.read_csv(f)[\"val_all\"].to_numpy() for f in files]\n",
    "curves_ot_val  = [pd.read_csv(f)[\"val_OT\"].to_numpy()  for f in files]\n",
    "curves_fp_val  = [pd.read_csv(f)[\"val_FP\"].to_numpy()  for f in files]\n",
    "times = pd.read_csv(files[0])[\"time_s\"].to_numpy()\n",
    "\n",
    "# group mean ± SEM\n",
    "gm_all, se_all = mean_sem(curves_all_val)\n",
    "gm_ot,  se_ot  = mean_sem(curves_ot_val)\n",
    "gm_fp,  se_fp  = mean_sem(curves_fp_val)\n",
    "\n",
    "# 1st plot: cumulative decoding with SEM\n",
    "plt.figure(figsize=(7.8, 4.4))\n",
    "\n",
    "for gm, se, lab, lw, alpha in [\n",
    "    (gm_all, se_all, \"All ROIs (val)\", 2.2, 0.18),\n",
    "    (gm_ot,  se_ot,  \"Occipito-temporal (val)\", 1.9, 0.18),\n",
    "    (gm_fp,  se_fp,  \"Frontal–parietal (val)\", 1.9, 0.18),\n",
    "]:\n",
    "    plt.plot(times, gm, label=lab, lw=lw)\n",
    "    plt.fill_between(times, gm - se, gm + se, alpha=alpha)\n",
    "\n",
    "plt.axvspan(VAN_WIN[0], VAN_WIN[1], color=\"grey\", alpha=0.15, label=\"VAN (150–250 ms)\")\n",
    "plt.axvspan(LP_WIN[0],  LP_WIN[1],  color=\"grey\", alpha=0.10, label=\"LP (330–550 ms)\")\n",
    "plt.axhline(chance, color=\"k\", ls=\"--\", lw=0.9, label=\"Chance (3-class)\")\n",
    "\n",
    "ymax = max(gm_all.max(), gm_ot.max(), gm_fp.max()) + 0.06\n",
    "plt.xlim(times[0], times[-1])\n",
    "plt.ylim(0.30, ymax)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Balanced accuracy (validation)\")\n",
    "plt.title(\"Cumulative decoding over time (group mean ± SEM)\")\n",
    "plt.legend(loc=\"lower right\", ncol=2, fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "f_val = os.path.join(OUT_DIR, \"group_cumulative_plot_val_sem.png\")\n",
    "plt.savefig(f_val, dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved:\", f_val)\n",
    "\n",
    "# 2nd plot: overfitting gap (mean ± SEM, All ROIs)\n",
    "curves_all_gap = [pd.read_csv(f)[\"gap_all\"].to_numpy() for f in files]\n",
    "gm_gap, se_gap = mean_sem(curves_all_gap)\n",
    "\n",
    "plt.figure(figsize=(7.8, 2.9))\n",
    "plt.plot(times, gm_gap, lw=2.2, label=\"All ROIs (mean gap)\")\n",
    "plt.fill_between(times, gm_gap - se_gap, gm_gap + se_gap, alpha=0.2)\n",
    "plt.axhline(0, color=\"k\", ls=\"--\", lw=0.9)\n",
    "plt.axvspan(VAN_WIN[0], VAN_WIN[1], color=\"grey\", alpha=0.15)\n",
    "plt.axvspan(LP_WIN[0],  LP_WIN[1],  color=\"grey\", alpha=0.10)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Train − Val\")\n",
    "plt.title(\"Group mean overfitting gap (± SEM)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "f_gap = os.path.join(OUT_DIR, \"group_overfit_gap_plot_sem.png\")\n",
    "plt.savefig(f_gap, dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved:\", f_gap)\n",
    "\n",
    "# onset and peak summaries \n",
    "ot_onset = first_exceed(times, gm_ot, base=chance, delta=0.02, min_dur=0.03)\n",
    "fp_onset = first_exceed(times, gm_fp, base=chance, delta=0.02, min_dur=0.03)\n",
    "\n",
    "all_peak_val, all_peak_t = peak_in_window(times, gm_all, (VAN_WIN[0], LP_WIN[1]))\n",
    "ot_peak_val,  ot_peak_t  = peak_in_window(times, gm_ot,  (VAN_WIN[0], LP_WIN[1]))\n",
    "fp_peak_val,  fp_peak_t  = peak_in_window(times, gm_fp,  (VAN_WIN[0], LP_WIN[1]))\n",
    "\n",
    "print(\n",
    "    f\"Onset > chance+0.02 for ≥30 ms — OT: {ot_onset*1000:.0f} ms, FP: {fp_onset*1000:.0f} ms\"\n",
    "    if (not np.isnan(ot_onset) and not np.isnan(fp_onset))\n",
    "    else \"Onset criterion not reached for at least one curve.\"\n",
    ")\n",
    "print(\n",
    "    f\"Peak (0.15–0.55 s) — All: {all_peak_val:.3f} @ {all_peak_t*1000:.0f} ms | \"\n",
    "    f\"OT: {ot_peak_val:.3f} @ {ot_peak_t*1000:.0f} ms | \"\n",
    "    f\"FP: {fp_peak_val:.3f} @ {fp_peak_t*1000:.0f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b22376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAN/LP window means + paired tests for Model 3 cumulative curves (might not use)\n",
    "import os, numpy as np, pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# paths and time windows\n",
    "OUT_DIR = \"/work/GrétaHarsányi#3675/Assignment2/2025Neuro/analysis_files/cumulative_fast\"\n",
    "VAN_WIN = (0.150, 0.250)\n",
    "LP_WIN  = (0.330, 0.550)\n",
    "\n",
    "files = sorted([f for f in os.listdir(OUT_DIR) if f.endswith(\"_cumulative_curves_overfit.csv\")])\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No cumulative curve CSVs found in {OUT_DIR}\")\n",
    "\n",
    "# er participanzt window means\n",
    "rows = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(OUT_DIR, f))\n",
    "    pid = f.split(\"_\")[0]\n",
    "    for roi in [\"acc_all\", \"acc_OT\", \"acc_FP\"]:\n",
    "        van_mean = df.loc[(df[\"time_s\"] >= VAN_WIN[0]) & (df[\"time_s\"] <= VAN_WIN[1]), roi].mean()\n",
    "        lp_mean  = df.loc[(df[\"time_s\"] >= LP_WIN[0])  & (df[\"time_s\"] <= LP_WIN[1]),  roi].mean()\n",
    "        rows.append({\"pid\": pid, \"roi\": roi, \"VAN\": van_mean, \"LP\": lp_mean})\n",
    "\n",
    "win_means = pd.DataFrame(rows).sort_values([\"roi\", \"pid\"]).reset_index(drop=True)\n",
    "\n",
    "# aving\n",
    "means_path = os.path.join(OUT_DIR, \"cumulative_window_means.csv\")\n",
    "win_means.to_csv(means_path, index=False)\n",
    "print(\"✓ Saved per-participant VAN/LP means →\", means_path)\n",
    "\n",
    "# aired tests: van vs lp for each ROI set\n",
    "tests = []\n",
    "for roi in [\"acc_all\", \"acc_OT\", \"acc_FP\"]:\n",
    "    sub = win_means[win_means[\"roi\"] == roi]\n",
    "    m = sub[[\"VAN\", \"LP\"]].dropna()\n",
    "    t, p = ttest_rel(m[\"VAN\"], m[\"LP\"])\n",
    "    tests.append({\n",
    "        \"roi\": roi,\n",
    "        \"n\": int(len(m)),\n",
    "        \"VAN_mean\": float(m[\"VAN\"].mean()),\n",
    "        \"VAN_sd\":   float(m[\"VAN\"].std(ddof=1)),\n",
    "        \"LP_mean\":  float(m[\"LP\"].mean()),\n",
    "        \"LP_sd\":    float(m[\"LP\"].std(ddof=1)),\n",
    "        \"t_VAN_vs_LP\": float(t),\n",
    "        \"p_VAN_vs_LP\": float(p)\n",
    "    })\n",
    "    print(f\"{roi}: VAN={m['VAN'].mean():.3f}±{m['VAN'].std(ddof=1):.3f} \"\n",
    "          f\"vs LP={m['LP'].mean():.3f}±{m['LP'].std(ddof=1):.3f} | \"\n",
    "          f\"t={t:.2f}, p={p:.4f}, n={len(m)}\")\n",
    "\n",
    "tests_df = pd.DataFrame(tests).sort_values(\"roi\")\n",
    "tests_path = os.path.join(OUT_DIR, \"cumulative_window_tests.csv\")\n",
    "tests_df.to_csv(tests_path, index=False)\n",
    "print(\"✓ Saved VAN vs LP paired-test summary →\", tests_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing time window means against chance and comparing OT vs FP within each window \n",
    "\n",
    "\n",
    "chance = 1/3\n",
    "\n",
    "def ci95(x):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n  = len(x)\n",
    "    m  = x.mean()\n",
    "    se = x.std(ddof=1) / np.sqrt(n)\n",
    "    tcrit = stats.t.ppf(0.975, n-1)\n",
    "    return m - tcrit*se, m + tcrit*se\n",
    "\n",
    "# t-tests\n",
    "rows_vs_chance = []\n",
    "for w in [\"VAN\", \"LP\"]:\n",
    "    print(f\"\\n== {w} window ==\")\n",
    "    for roi in [\"acc_all\",\"acc_OT\",\"acc_FP\"]:\n",
    "        vals = win_means.loc[win_means[\"roi\"]==roi, w].astype(float).dropna().values\n",
    "        t, p = stats.ttest_1samp(vals, chance)\n",
    "        lo, hi = ci95(vals)\n",
    "        m = float(vals.mean())\n",
    "        print(f\"{roi:8s}: mean={m:.3f}  95%CI[{lo:.3f},{hi:.3f}]  t={t:.2f}, p={p:.4f}  (n={len(vals)})\")\n",
    "        rows_vs_chance.append({\"window\": w, \"roi\": roi, \"n\": len(vals),\n",
    "                               \"mean\": m, \"ci95_low\": lo, \"ci95_high\": hi,\n",
    "                               \"t\": float(t), \"p\": float(p)})\n",
    "\n",
    "# paired OT vs FP\n",
    "rows_pair = []\n",
    "for w in [\"VAN\",\"LP\"]:\n",
    "    sub = win_means[win_means[\"roi\"].isin([\"acc_OT\",\"acc_FP\"])][[\"pid\",\"roi\",w]].dropna()\n",
    "    wide = sub.pivot(index=\"pid\", columns=\"roi\", values=w).dropna()\n",
    "    ot = wide[\"acc_OT\"].values\n",
    "    fp = wide[\"acc_FP\"].values\n",
    "    t, p = stats.ttest_rel(ot, fp)\n",
    "    print(f\"OT vs FP ({w}): t={t:.2f}, p={p:.4f}  (n={len(wide)})\")\n",
    "    rows_pair.append({\"window\": w, \"n\": len(wide), \"t\": float(t), \"p\": float(p)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
